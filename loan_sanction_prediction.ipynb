{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723873090109},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723778725460},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723720997842},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723702539126},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723629465004},{"file_id":"1IG3rtuSryUSW9aJSzSv3Cj9Q3sjlK-hV","timestamp":1723606680546},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723550355534},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723525668579}],"collapsed_sections":["6U4S3d7fsRLq","fcYXDXxxbYzn","hrPFXTgf0z2m","gWXVdU3rhoRK","UUPAWn7ktONj","I1DWjlExrRyn","TVgjxyQz01mx","TvlrWpey4jBW","xx72OX6cJeef","4YyR3UMbJlEt","NvBgnlC3Mgb9","OsxHF1Sx4tgS","1iHpN_dsu6ob","rH3hH5rHvwlJ","kF98C-5GwPWy","7BMKGNEuwXy0","fXsc2GPmvXRd","nIYgODFCznju","3DtS8s2631xQ","tOkesqzLJQye","IubF6rcAJa9i","4e56KVp79hW1"],"authorship_tag":"ABX9TyNRHcXDqzDSKLk4cW6SYezT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Configuring libraries & utilities"],"metadata":{"id":"6U4S3d7fsRLq"}},{"cell_type":"code","source":["# Import required libraries\n","import os\n","import json\n","import math\n","\n","from google.colab import drive\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","from xgboost import XGBClassifier\n","from sklearn.svm import SVC\n","from sklearn.impute import SimpleImputer\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",")\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",")\n","import pickle as pkl\n","\n","\n","# Try to import the custom module\n","try:\n","    import flash as fz\n","except ImportError:\n","    # Install the module if not present\n","    !pip install git+https://github.com/Althaf9900/flash.git    # Feel free to contribute to our custom module :)\n","    import flash as fz"],"metadata":{"id":"I1_atIHsa4Br"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive if it's not already mounted\n","mount_point = \"/content/drive\"\n","\n","if not os.path.ismount(mount_point):\n","    print(\"Mounting Google Drive...\")\n","    drive.mount(mount_point)\n","else:\n","    print(\"Google Drive is already mounted.\")\n","\n","%cd /content/drive/MyDrive/Projects/loan-sanction-prediction"],"metadata":{"collapsed":true,"id":"WRozjl381lRa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Initial data assessment & preparation\n"],"metadata":{"id":"fcYXDXxxbYzn"}},{"cell_type":"code","source":["# Loading the dataset\n","df_copy = pd.read_csv('loan_sanction_train.csv')\n","df = df_copy"],"metadata":{"id":"GaiVFZ039qYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Understanding structure of the dataset\n","df.sample(5)"],"metadata":{"id":"--gXs5jR_W5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking whether Loan_ID contains duplicate IDs\n","df['Loan_ID'].duplicated().sum()"],"metadata":{"id":"eAjHUDF4VQ3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking whether the dataset is imbalanced or not\n","plt.pie(df['Loan_Status'].value_counts(), labels = df['Loan_Status'].unique(), autopct='%0.2f%%',\n","        shadow=True, explode=(0, 0.1), counterclock=False, colors=['lime', 'cyan'])\n","plt.show()"],"metadata":{"id":"ry1walf3AM55"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Columns of the dataset\n","print(df.columns)"],"metadata":{"id":"6GjPVOWM2il5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting some information about the dataset\n","df.info()"],"metadata":{"id":"lUPbGCTr_b20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dropping useless features\n","df.drop('Loan_ID', axis=1, inplace=True)"],"metadata":{"id":"G2JG-P6ldwgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting numerical features & categorical features from the dataset using a custom made module\n","num_cols = fz.get_num_col(df)\n","cat_cols = fz.get_cat_col(df, ignore_cols=['Loan_Status'])\n","\n","# Number of categorical features\n","n_cat_cols = len(cat_cols)\n","n_num_cols = len(num_cols)\n","\n","\n","print(num_cols)\n","print(cat_cols)\n","\n","print(f'Number of numerical features: {n_num_cols}')\n","print(f'Number of categorical features: {n_cat_cols}')"],"metadata":{"id":"8wFiCJgV_eIx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (Before data cleaning)\n"],"metadata":{"id":"hrPFXTgf0z2m"}},{"cell_type":"markdown","source":["\n","## Outlier analysis on numerical features\n"],"metadata":{"id":"K6aKkPNA24wd"}},{"cell_type":"code","source":["# Statistical measures\n","df[num_cols].describe().T"],"metadata":{"id":"TB8kLnYq4TRX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histogram & Box-plot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 10))\n","\n","# Plotting histograms and boxplots\n","for i, col in enumerate(num_cols):\n","    # Histogram\n","    sns.histplot(df[col], kde=True, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Histogram of {col}')\n","\n","    # Boxplot\n","    sns.boxplot(data=df, x=col, ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Boxplot of {col}')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"CmE1HHsT4v0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize a list to store features with outliers\n","ftrs_with_outliers = []\n","\n","# Iterate through each numerical features to calculate IQR and identify outliers\n","for feature in df[num_cols].columns:\n","    # Calculate Q1, Q3, and IQR\n","    Q1 = df[feature].quantile(0.25)\n","    Q3 = df[feature].quantile(0.75)\n","    IQR = Q3 - Q1\n","\n","    # Define outlier bounds\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","\n","    # Check if the feature contains any outliers\n","    if ((df[feature] < lower_bound) | (df[feature] > upper_bound)).any():\n","        ftrs_with_outliers.append(feature)\n","\n","print(\"Features with outliers:\", ftrs_with_outliers)"],"metadata":{"id":"BgqOq69_VSzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applicant income\n","Q1 = df['ApplicantIncome'].quantile(0.25)\n","Q3 = df['ApplicantIncome'].quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","df[(df['ApplicantIncome'] < lower_bound) | (df['ApplicantIncome'] > upper_bound)]['ApplicantIncome'].sort_values()"],"metadata":{"id":"tqfslX_KW2Hw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Co-applicant income\n","Q1 = df['CoapplicantIncome'].quantile(0.25)\n","Q3 = df['CoapplicantIncome'].quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","df[(df['CoapplicantIncome'] < lower_bound) | (df['CoapplicantIncome'] > upper_bound)]['CoapplicantIncome'].sort_values()"],"metadata":{"id":"cnpLUQyqFVdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loan amount\n","Q1 = df['LoanAmount'].quantile(0.25)\n","Q3 = df['LoanAmount'].quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","df[(df['LoanAmount'] < lower_bound) | (df['LoanAmount'] > upper_bound)]['LoanAmount'].sort_values()"],"metadata":{"id":"NQlfiS8zFg4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- There are many outliers on the upper side of all numerical features.\n","\n","- None of the numerical features have outliers on the lower side.\n","\n","- Since we only have few data points, we can't afford to drop any data points.\n","    \n","- None of the numerical features follow a normal distribution.\n","\n","- The outliers appear to be valid and are not due to data entry issues.\n","\n","- Since the outliers are valid, apply capping methods, such as:\n","\n","    - Custom threshold capping: Set a threshold value based on analysis of the boxplots.\n","    - Percentile-based capping: Limit outliers to a specified percentile range.\n","    - Median imputation: Replace extreme values with the median.\n","\n","- After building the predictive model, evaluate the accuracy of all capping methods."],"metadata":{"id":"Nxw1gMdUdRoH"}},{"cell_type":"markdown","source":["\n","## Missing value analysis\n"],"metadata":{"id":"gWXVdU3rhoRK"}},{"cell_type":"code","source":["# Calculate the percentage of missing values in numerical features\n","num_miss_pct = df[num_cols].isna().mean()*100\n","\n","# Filter out categorical features with no missing values\n","num_miss_pct = num_miss_pct[num_miss_pct > 0]\n","\n","# Print the percentage of missing values for each numerical features with missing values\n","for index, value in num_miss_pct.items():\n","    print(f\"{index}: {round(value, 2)}%\")\n","\n","# List of numerical features that have missing values\n","num_cols_with_na = num_miss_pct.index.to_list()\n","\n","print(num_cols_with_na)"],"metadata":{"id":"A7Db6Hti_wi_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the percentage of missing values in categorical features\n","cat_miss_pct = df[cat_cols].isna().mean() * 100\n","\n","# Filter out categorical features with no missing values\n","cat_miss_pct = cat_miss_pct[cat_miss_pct > 0]\n","\n","# Print the percentage of missing values for each categorical feature with missing values\n","for feature, pct in cat_miss_pct.items():\n","    print(f\"{feature}: {round(pct, 2)}%\")\n","\n","# List of categorical features that have missing values\n","cat_cols_with_na = cat_miss_pct.index.to_list()\n","\n","print(cat_cols_with_na)"],"metadata":{"id":"bYOfY62oceGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing whether the missing values are missing at random or not\n","plt.figure(figsize=(15, 4))\n","sns.heatmap(df.isna(), cbar=False, cmap=\"Blues\", yticklabels=False)\n","plt.xticks(rotation=45)\n","plt.show()"],"metadata":{"id":"buU_pHC2WwQg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting histogram of numerical features that have missing values to decide whether to use mean or median\n","sns.histplot(df['LoanAmount'], kde=True)\n","plt.show()"],"metadata":{"id":"eWwW3tdPcD1c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- Only one numerical feature (['LoanAmount']) has missing values.\n","\n","- Six categorical features (['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History']) have missing values.\n","\n","- Since we only have few data points, we cannot afford to drop any data points.\n","\n","- The percentage of missing values is low across all features, so there is no need to drop any columns.\n","\n","- It appears that the missingness of values is random.\n","\n","- Missing value handling:\n","\n","    - Median imputation (for numerical features that are not normally distributed):\n","        - Loan amount\n","\n","    - Mode imputation:\n","        - Categorical features"],"metadata":{"id":"2Jvk--P3iawd"}},{"cell_type":"markdown","source":["\n","# Data Cleaning\n"],"metadata":{"id":"UUPAWn7ktONj"}},{"cell_type":"markdown","source":["Data cleaning conclusions:\n","\n","- Outlier handling:\n","\n","    - Since the outliers are valid, apply capping methods, such as:\n","\n","        - Custom threshold capping: Set a threshold value based on analysis of the boxplots.\n","        - Percentile-based capping: Limit outliers to a specified percentile range.\n","        - Median imputation: Replace outliers with the median.\n","\n","\n","- Missing value handling:\n","\n","    - Median imputation (for numerical features that are not normally distributed):\n","        - Loan amount\n","\n","    - Mode imputation:\n","        - Categorical features\n","\n","- Data type adjustments:\n","\n","    - Data type comapatibility:\n","        - Applicant income: float\n","        - Loan_Amount_Term: int then, str\n","        - Credit_History: int then, str\n","\n","    - Memory usage optimization:\n","        - Categorical features: category\n","        - Numerical features: float64 -> float32"],"metadata":{"id":"rsFPXBYzVz27"}},{"cell_type":"code","source":["# Outlier handling: Custom threshold capping\n","\n","# Define the cap value\n","cap_values = [20833, 8980, 500]\n","\n","for i, feature in enumerate(ftrs_with_outliers):\n","    # Cap the values\n","    df[feature] = df[feature].clip(upper=cap_values[i])"],"metadata":{"id":"arnnaMYTD-UE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Missing value handling\n","\n","# Imputing missing values in LoanAmount feature with median\n","median_imputer = SimpleImputer(strategy='median')\n","df['LoanAmount'] = median_imputer.fit_transform(df[['LoanAmount']])\n","\n","# Imputing missing values in categorical features with mode\n","mode_imputer = SimpleImputer(strategy='most_frequent')\n","df[cat_cols_with_na] = mode_imputer.fit_transform(df[cat_cols_with_na])\n","\n","# Test\n","if df.isna().sum().sum() == 0:\n","    print(\"There are no missing values left in the DataFrame.\")\n","else:\n","    print(\"There are still missing values in the DataFrame.\")"],"metadata":{"id":"0INL_0mEih5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking memory usage before dtype adjustments\n","print(\"Memory usage before adjustment:\", df.memory_usage(deep=True).sum())\n","print()\n","\n","# Data type adjustments\n","\n","# Data type compatibility\n","df['ApplicantIncome'] = df['ApplicantIncome'].astype(float)\n","\n","# Converting numerical categorical features to int\n","df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype(int)\n","df['Credit_History'] = df['Credit_History'].astype(int)\n","\n","# Converting numerical categorical features to str\n","df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype(str)\n","df['Credit_History'] = df['Credit_History'].astype(str)\n","\n","# Memory usage optimization\n","df[cat_cols] = df[cat_cols].astype('category')\n","df[num_cols] = df[num_cols].astype(np.float32)\n","\n","# Print data types to confirm changes\n","print(df.dtypes)\n","\n","# Checking memory usage after dtype adjustments\n","print()\n","print(\"Memory usage after adjustment:\", df.memory_usage(deep=True).sum())"],"metadata":{"id":"kqpDRtGPlVJo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (After data cleaning)\n"],"metadata":{"id":"I1DWjlExrRyn"}},{"cell_type":"markdown","source":["\n","## Univariate analysis\n"],"metadata":{"id":"TVgjxyQz01mx"}},{"cell_type":"markdown","source":["\n","### Numerical features\n"],"metadata":{"id":"NgEA39dklZ8N"}},{"cell_type":"code","source":["df[num_cols].describe().T"],"metadata":{"id":"mDi3hcLNlfpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Skewness and Kurtosis\n","skew_kurt_dict = {}\n","for col in num_cols:\n","    skew_kurt_dict[col] = {\n","        'skewness': round(float(df[col].skew()), 2),\n","        'kurtosis': round(float(df[col].kurtosis()), 2)\n","    }\n","\n","print(json.dumps(skew_kurt_dict, indent=4))"],"metadata":{"id":"QOOWv8wfp3Px"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histogram & Box-plot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 10))\n","\n","# Plotting histograms and boxplots\n","for i, col in enumerate(num_cols):\n","    # Histogram\n","    sns.histplot(df[col], kde=True, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Histogram of {col}')\n","\n","    # Boxplot\n","    sns.boxplot(data=df, x=col, ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Boxplot of {col}')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"B4nhzoKGmhYz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- Applicant income and loan amount approximately follow a log-normal distribution.\n","- Feature transformation is needed for all numerical features.\n","- It appears that individuals with a co-applicant income of 0 do not have a co-applicant. Therefore, create a feature named 'Has_coapplicant'. In this feature, set individuals with a co-applicant income of 0 to 'No, and those with a non-zero co-applicant income to 'Yes."],"metadata":{"id":"-A0YZbXsnK3C"}},{"cell_type":"markdown","source":["\n","### Categorical features\n"],"metadata":{"id":"cRUOZJ-l2h9s"}},{"cell_type":"code","source":["# Statistical measures\n","df[cat_cols].describe().T"],"metadata":{"id":"LkOlLNwQ32Q_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Countplot\n","\n","# Calculate number of rows and columns needed for subplots\n","n_cols = 3\n","n_rows = math.ceil(n_cat_cols / n_cols)\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n","\n","# Flatten axs array if it's multidimensional\n","axs = axs.flatten()\n","\n","# Plot countplots and set titles\n","for i, feature in enumerate(cat_cols):\n","    sns.countplot(data=df, x=feature, ax=axs[i])\n","    axs[i].set_title(feature)\n","    axs[i].tick_params(axis='x', rotation=45)\n","\n","# Turn off any unused subplots\n","for j in range(len(cat_cols), len(axs)):\n","    axs[j].axis('off')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"86PJgdX_3uoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- Males take out more loans compared to females.\n","- Married individuals take out more loans compared to - unmarried individuals.\n","- People without dependents take out more loans compared to those with dependents.\n","- Graduates take out more loans compared to non-graduates.\n","- Non-self-employed individuals take out more loans compared to self-employed individuals.\n","- Most people opt for a loan term of 360 months (30 years), followed by 180 months (15 years).\n","- People with a credit history of 1 take out more loans compared to those with a credit history of 0.\n","- People living in semi-urban areas take out more loans compared to those living in rural and urban areas. Rural residents take out the fewest loans. Although these relationships aren't strong, they may represent general trends."],"metadata":{"id":"Z1pkXN3GhmYk"}},{"cell_type":"markdown","source":["\n","## Bivariate analysis\n"],"metadata":{"id":"TvlrWpey4jBW"}},{"cell_type":"markdown","source":["\n","### Between independent features\n"],"metadata":{"id":"f5oxalLjrgOO"}},{"cell_type":"markdown","source":["\n","#### Numerical - Numerical\n"],"metadata":{"id":"xx72OX6cJeef"}},{"cell_type":"code","source":["# Pairplot\n","sns.pairplot(df[num_cols], diag_kind='kde')\n","plt.show()"],"metadata":{"id":"82F8jFg-8xI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# regplot with PairGrid\n","graph = sns.PairGrid(df)\n","\n","# type of graph for diagonal\n","graph = graph.map_diag(sns.histplot, kde=True)\n","\n","# type of graph for non-diagonal\n","graph = graph.map_offdiag(sns.regplot, scatter=True, line_kws={'color': 'red'})\n","\n","plt.show()"],"metadata":{"id":"B8ShB9vZA3tU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a custom colorbar\n","# Define custom colors\n","colors = [\"#FF0000\", \"#FFFF00\", \"#00FF00\"]  # Red to Yellow to Green\n","\n","# Create a custom colormap\n","custom_cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", colors)"],"metadata":{"id":"xiZWa00gqpn8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to create a mask for the upper triangle\n","def create_mask(df, method):\n","    corr = df.corr(method=method)\n","    mask = np.triu(np.ones_like(corr, dtype=bool))\n","    np.fill_diagonal(mask, False)  # Optional: keep or remove diagonal elements\n","    return mask\n","\n","# Function to plot heatmap\n","def plot_heatmap(df, method, ax, cmap, title):\n","    mask = create_mask(df, method)\n","    sns.heatmap(df.corr(method=method), mask=mask, annot=True, cmap=cmap, ax=ax, cbar=False)\n","    ax.set_title(title)"],"metadata":{"id":"QS8prjqVxyGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(1, 3, figsize=(13, 5), gridspec_kw={'width_ratios': [1, 1, 0.05]})\n","\n","# Plot Pearson and Spearman heatmaps\n","plot_heatmap(df[num_cols], 'pearson', axs[0], custom_cmap, 'Pearson Correlation Heatmap')\n","plot_heatmap(df[num_cols], 'spearman', axs[1], custom_cmap, 'Spearman Correlation Heatmap')\n","\n","# Create a common colorbar for both heatmaps\n","cbar = fig.colorbar(axs[0].collections[0], cax=axs[2])\n","\n","# Adjust layout to prevent overlapping\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"WySczazSJ0AZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- None of the features show a strong linear relationship with each other. However, there is a moderate relationship between applicant income and loan amount. This makes sense because individuals with higher incomes often need larger loan amounts.\n","\n","- Both Pearson and Spearman correlation coefficients show similar patterns, but their values are slightly different. Since the heatmaps from both are similar, the exact values are less important. In this case, Spearman's correlation is more suitable because the data isn't normally distributed, doesn't have a linear relationship between features, and has outliers."],"metadata":{"id":"e87zhIkG2MKg"}},{"cell_type":"markdown","source":["\n","#### Categorical - Categorical\n"],"metadata":{"id":"4YyR3UMbJlEt"}},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols * (n_cat_cols - 1) // 2, 2, figsize=(13, 140))  # Adjust size as needed\n","axs = axs.reshape(-1, 2)  # Flatten the array of subplots\n","\n","# Plot heatmaps\n","plot_index = 0\n","for i in range(n_cat_cols):\n","    for j in range(i + 1, n_cat_cols):\n","        # Create contingency tables\n","        contingency_table_1 = pd.crosstab(df[cat_cols[i]], df[cat_cols[j]], normalize='index') * 100\n","        contingency_table_2 = pd.crosstab(df[cat_cols[i]], df[cat_cols[j]], normalize='columns') * 100\n","\n","        # Plot heatmaps\n","        sns.heatmap(contingency_table_1, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                    xticklabels=True, yticklabels=True, ax=axs[plot_index, 0])\n","        sns.heatmap(contingency_table_2, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                    xticklabels=True, yticklabels=True, ax=axs[plot_index, 1])\n","\n","        # Set titles for each subplot\n","        axs[plot_index, 0].set_title(f\"{cat_cols[i]} vs {cat_cols[j]} (Index Normalized)\")\n","        axs[plot_index, 1].set_title(f\"{cat_cols[i]} vs {cat_cols[j]} (Column Normalized)\")\n","\n","        plot_index += 1\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"64_F3iSAsNP1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### Numerical - Categorical\n"],"metadata":{"id":"NvBgnlC3Mgb9"}},{"cell_type":"code","source":["# Applicant income\n","num_col = 'ApplicantIncome'\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(15, n_cat_cols * 4))\n","\n","for i in range(len(cat_cols)):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x=cat_cols[i], y=num_col, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_col} by {cat_cols[i]}')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_col} by {cat_cols[i]}')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"T3Ri0qCZ7S9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Coapplicant income\n","num_col = 'CoapplicantIncome'\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(15, n_cat_cols * 4))\n","\n","for i in range(len(cat_cols)):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x=cat_cols[i], y=num_col, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_col} by {cat_cols[i]}')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_col} by {cat_cols[i]}')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"j_PPBJqN9O_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loan amount\n","num_col = 'LoanAmount'\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(15, n_cat_cols * 4))\n","\n","for i in range(len(cat_cols)):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x=cat_cols[i], y=num_col, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_col} by {cat_cols[i]}')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_col} by {cat_cols[i]}')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"gyNZz0No9SFq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Between independent features and target feature\n"],"metadata":{"id":"M3XQd3-ArtIg"}},{"cell_type":"markdown","source":["\n","#### Categorical - Categorical"],"metadata":{"id":"taXwxKdlsWjj"}},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(13, 40))  # Adjust size as needed\n","axs = axs.reshape(-1, 2)  # Flatten the array of subplots\n","\n","# Plot heatmaps\n","for i in range(n_cat_cols):\n","    # Create contingency tables\n","    contingency_table_1 = pd.crosstab(df[cat_cols[i]], df['Loan_Status'], normalize='index') * 100\n","    contingency_table_2 = pd.crosstab(df[cat_cols[i]], df['Loan_Status'], normalize='columns') * 100\n","\n","    # Plot heatmaps\n","    sns.heatmap(contingency_table_1, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 0])\n","    sns.heatmap(contingency_table_2, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 1])\n","\n","    # Set titles for each subplot\n","    axs[i, 0].set_title(f\"{cat_cols[i]} vs Loan_Status (Index Normalized)\")\n","    axs[i, 1].set_title(f\"{cat_cols[i]} vs Loan_Status (Column Normalized)\")\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"ubUDwj0psXk_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### Numerical - Categorical\n"],"metadata":{"id":"ygTp4kz1r4Jp"}},{"cell_type":"code","source":["# Violinplot & Pointplot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 9))\n","\n","for i in range(n_num_cols):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x='Loan_Status', y=num_cols[i], ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_cols[i]} by Loan_Status')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x='Loan_Status', y=num_cols[i], errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x='Loan_Status', y=num_cols[i], errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_cols[i]} by Loan_Status')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"SE3nsx3crznT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Feature engineering\n"],"metadata":{"id":"OsxHF1Sx4tgS"}},{"cell_type":"markdown","source":["\n","## Feature transformation"],"metadata":{"id":"M1KAAAvL4ht-"}},{"cell_type":"markdown","source":["\n","## Feature construction\n"],"metadata":{"id":"1iHpN_dsu6ob"}},{"cell_type":"markdown","source":["- It appears that individuals with a co-applicant income of 0 do not have a co-applicant. Therefore, create a feature named 'Has_coapplicant'. In this feature, set individuals with a co-applicant income of 0 to 'No, and those with a non-zero co-applicant income to 'Yes."],"metadata":{"id":"DilMLweTueyk"}},{"cell_type":"code","source":["df['Has_coapplicant'] = np.where(df['CoapplicantIncome'] == 0, 'No', 'Yes')\n","df['Has_coapplicant']"],"metadata":{"id":"Op8pR4w8kIr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"MQPkByxiL-nF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (On the basis of newly constructed features)\n"],"metadata":{"id":"rH3hH5rHvwlJ"}},{"cell_type":"markdown","source":["\n","## Univariate analysis\n"],"metadata":{"id":"kF98C-5GwPWy"}},{"cell_type":"code","source":["df['Has_coapplicant'].describe().T"],"metadata":{"id":"TLhb427hwUCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.countplot(x = df['Has_coapplicant'])\n","plt.show()"],"metadata":{"id":"dAiY4fGRwXHK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Bivariate analysis\n"],"metadata":{"id":"7BMKGNEuwXy0"}},{"cell_type":"markdown","source":["\n","### Between independent features and newly constructed features\n"],"metadata":{"id":"fXsc2GPmvXRd"}},{"cell_type":"markdown","source":["\n","#### Categorical - Categorical"],"metadata":{"id":"pQteagYGy5b_"}},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(13, 40))  # Adjust size as needed\n","axs = axs.reshape(-1, 2)  # Flatten the array of subplots\n","\n","# Plot heatmaps\n","for i in range(n_cat_cols):\n","    # Create contingency tables\n","    contingency_table_1 = pd.crosstab(df[cat_cols[i]], df['Has_coapplicant'], normalize='index') * 100\n","    contingency_table_2 = pd.crosstab(df[cat_cols[i]], df['Has_coapplicant'], normalize='columns') * 100\n","\n","    # Plot heatmaps\n","    sns.heatmap(contingency_table_1, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 0])\n","    sns.heatmap(contingency_table_2, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 1])\n","\n","    # Set titles for each subplot\n","    axs[i, 0].set_title(f\"{cat_cols[i]} vs Has_coapplicant (Index Normalized)\")\n","    axs[i, 1].set_title(f\"{cat_cols[i]} vs Has_coapplicant (Column Normalized)\")\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"pIGdHXidlZEw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### Numerical - Categorical\n"],"metadata":{"id":"LJ7rmAjWzHzZ"}},{"cell_type":"code","source":["# Violinplot & Pointplot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 9))\n","\n","for i in range(n_num_cols):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x='Has_coapplicant', y=num_cols[i], ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_cols[i]} by Has_coapplicant')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x='Has_coapplicant', y=num_cols[i], errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x='Has_coapplicant', y=num_cols[i], errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_cols[i]} by Has_coapplicant')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TUk79SjZy3oM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Between newly constructed features and target feature\n"],"metadata":{"id":"nIYgODFCznju"}},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # Adjust size as needed\n","axs = axs.reshape(-1, 2)  # Flatten the array of subplots\n","\n","# Plot heatmaps\n","for i in range(1):\n","    # Create contingency tables\n","    contingency_table_1 = pd.crosstab(df['Has_coapplicant'], df['Loan_Status'], normalize='index') * 100\n","    contingency_table_2 = pd.crosstab(df['Has_coapplicant'], df['Loan_Status'], normalize='columns') * 100\n","\n","    # Plot heatmaps\n","    sns.heatmap(contingency_table_1, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 0])\n","    sns.heatmap(contingency_table_2, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 1])\n","\n","    # Set titles for each subplot\n","    axs[i, 0].set_title(f\"Has_coapplicant vs Loan_Status (Index Normalized)\")\n","    axs[i, 1].set_title(f\"Has_coapplicant vs Loan_Status (Column Normalized)\")\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"JYwtSeBoz0Wj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Building predictive model"],"metadata":{"id":"QR2ei6OxzBTw"}},{"cell_type":"markdown","source":["\n","## Preparing the data\n"],"metadata":{"id":"3DtS8s2631xQ"}},{"cell_type":"code","source":["# Appending newly constructed features to feature lists based on their type\n","cat_cols.append('Has_coapplicant')"],"metadata":{"id":"PNYmhFer_6bD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting the dataset into training and testing data\n","X_train, X_test, y_train, y_test = train_test_split(df.drop('Loan_Status', axis=1),\n","                                                    df['Loan_Status'], test_size=0.2,\n","                                                    random_state=42)"],"metadata":{"id":"_FnWrATH7LQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# One hot encode categorical features of train data\n","\n","# Initialize OneHotEncoder\n","ohe = OneHotEncoder(drop='first', sparse_output=False)\n","\n","# Fit and transform the training data\n","X_train_cat_encoded = ohe.fit_transform(X_train[cat_cols])\n","X_train_cat_encoded"],"metadata":{"id":"bM2ytO-q4MPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train[cat_cols]"],"metadata":{"id":"9QdKPtGBQReC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scale numerical features of the train data\n","\n","# Initialize StandardScaler\n","scaler = StandardScaler()\n","\n","# Fit and transform the train data\n","X_train_num_scaled = scaler.fit_transform(X_train[num_cols])\n","X_train_num_scaled"],"metadata":{"id":"YOtmKwgy3tOP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Concatenating categorical encoded categorical features and scaled numerical features to a single numpy array\n","X_train_encoded_and_scaled = np.concatenate([X_train_num_scaled, X_train_cat_encoded], axis=1)\n","X_train_encoded_and_scaled"],"metadata":{"id":"QIU0RsxlMZDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Label encode the target feature in train data\n","\n","# Initialize the LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit and transform the target feature\n","y_train = le.fit_transform(y_train)\n","y_train"],"metadata":{"id":"kvdJhMl55rMS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# One hot encode categorical features in test data\n","X_test_cat_encoded = ohe.transform(X_test[cat_cols])\n","\n","# Scale numerical features in test data using standardization\n","X_test_num_scaled = scaler.transform(X_test[num_cols])\n","\n","# Concatenate encoded and scaled data\n","X_test_encoded_and_scaled = np.concatenate([X_test_num_scaled, X_test_cat_encoded], axis=1)\n","X_test_encoded_and_scaled"],"metadata":{"id":"NDqU5f69QTc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Label encode target feature of test data\n","y_test = le.transform(y_test)"],"metadata":{"id":"5T4C8CIgSRaR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Model selection (Before hyperparameter tuning)\n"],"metadata":{"id":"tOkesqzLJQye"}},{"cell_type":"code","source":["# Initialize models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'Support Vector Machine': SVC(),\n","    'KNN': KNeighborsClassifier(),\n","    'Decision Trees': DecisionTreeClassifier(),\n","    'Xgboost': XGBClassifier(),\n","    'Extra Trees': ExtraTreesClassifier()\n","}\n","\n","# Define metrics\n","scoring_metrics = {\n","    'accuracy_score': make_scorer(accuracy_score),\n","    'precision_score': make_scorer(precision_score, average='macro'),\n","    'recall_score': make_scorer(recall_score, average='macro'),\n","    'f1_score': make_scorer(f1_score, average='macro')\n","}"],"metadata":{"id":"MQgmPbGs9Qwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_models(models, scoring_metrics, X_train, y_train):\n","    metrics_across_models = {model_name: {} for model_name in models.keys()}\n","    models_across_metrics = {metric_name: {} for metric_name in scoring_metrics.keys()}\n","\n","    for model_name, model in models.items():\n","        for metric_name, scorer in scoring_metrics.items():\n","            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scorer)\n","            cv_scores_mean = cv_scores.mean()\n","\n","            metrics_across_models[model_name][metric_name] = round(cv_scores_mean, 3)\n","            models_across_metrics[metric_name][model_name] = round(cv_scores_mean, 3)\n","\n","    sorted_models_across_metrics = {\n","        metric_name: dict(sorted(model_scores.items(), key=lambda item: item[1], reverse=True))\n","        for metric_name, model_scores in models_across_metrics.items()\n","    }\n","\n","    return metrics_across_models, sorted_models_across_metrics"],"metadata":{"id":"NCXNRkQqoQcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_across_models, sorted_models_across_metrics = evaluate_models(\n","    models, scoring_metrics, X_train_encoded_and_scaled, y_train\n",")"],"metadata":{"id":"IrKrW_YJoy3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Comparing metrics across models\n","print(json.dumps(metrics_across_models, indent=4))"],"metadata":{"id":"ARzQsfocvwGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Comparing models across metrics\n","print(json.dumps(sorted_models_across_metrics, indent=4))"],"metadata":{"id":"mJOR0ECoDc2X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","After evaluating the metrics, I have decided to focus on the top 4 models: Logistic Regression, Random Forest Classifier, Support Vector Classifier (SVC), and Extra Trees Classifier. These models have demonstrated strong performance across the different metrics, making them the best candidates for further fine-tuning and optimization."],"metadata":{"id":"9_21ZbTWH8dq"}},{"cell_type":"markdown","source":["\n","## Hyperparameter tuning\n"],"metadata":{"id":"IubF6rcAJa9i"}},{"cell_type":"code","source":["# Initialize models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Support Vector Machine': SVC(),\n","    'Extra Trees': ExtraTreesClassifier()\n","}"],"metadata":{"id":"AfG2BpNjKF4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define parameter grids for models\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10],\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['liblinear']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [50, 100, 200],\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4]\n","    },\n","    'Support Vector Machine': {\n","        'C': [0.1, 1, 10],\n","        'kernel': ['linear', 'rbf'],\n","        'gamma': ['scale', 'auto']\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'bootstrap': [True, False]\n","    }\n","}"],"metadata":{"id":"nv1r5gjDTo_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def perform_grid_search(models, param_grids, X_train, y_train):\n","    best_params = {}\n","    for model_name, model in models.items():\n","        print(f\"Processing {model_name}...\")\n","        param_grid = param_grids[model_name]\n","        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n","        grid_search.fit(X_train, y_train)\n","        best_params[model_name] = {\n","            'Best Parameters': grid_search.best_params_,\n","            'Best Score': round(grid_search.best_score_, 3)\n","        }\n","    return best_params"],"metadata":{"id":"yvIm51meOyWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_params = perform_grid_search(models, param_grids, X_train_encoded_and_scaled, y_train)\n","\n","# Print best results from grid search\n","print(json.dumps(best_params, indent=4))"],"metadata":{"id":"uyYCLkcQpMrP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize models for final evaluation\n","models_for_final_evaluation = {\n","    'Logistic Regression': LogisticRegression,\n","    'Random Forest': RandomForestClassifier,\n","    'Support Vector Machine': SVC,\n","    'Extra Trees': ExtraTreesClassifier\n","}\n","\n","metrics = {\n","    'accuracy': accuracy_score,\n","    'precision': precision_score,\n","    'recall': recall_score,\n","    'f1': f1_score\n","}"],"metadata":{"id":"VeMR_LZhfKEg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_best_models(best_params, models, X_train, X_test, y_train, y_test, metrics):\n","    results = {model_name: {metric_name: 0 for metric_name in metrics.keys()} for model_name in best_params.keys()}\n","\n","    for model_name, best_params in best_params.items():\n","        model_cls = models[model_name]\n","        model = model_cls(**best_params['Best Parameters'])\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        for metric_name, metric in metrics.items():\n","            results[model_name][metric_name] = metric(y_test, y_pred)\n","\n","    return results\n","\n","results = evaluate_best_models(\n","    best_params, models_for_final_evaluation, X_train_encoded_and_scaled,\n","    X_test_encoded_and_scaled, y_train, y_test, metrics\n","    )\n","\n","# Print final model accuracies\n","print(json.dumps(results, indent=4))"],"metadata":{"id":"ERe8tJ6PrNxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["log_reg_clf = LogisticRegression(**best_params['Logistic Regression']['Best Parameters'])\n","log_reg_clf.fit(X_train_encoded_and_scaled, y_train)"],"metadata":{"id":"n8BoXQueDYD0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.ensemble import VotingClassifier\n","\n","# # Initialize base models\n","# log_reg = LogisticRegression(**best_params['Logistic Regression']['Best Parameters'])\n","# rf = RandomForestClassifier(**best_params['Random Forest']['Best Parameters'])\n","# svc = SVC(**best_params['Support Vector Machine']['Best Parameters'])\n","# et = ExtraTreesClassifier(**best_params['Extra Trees']['Best Parameters'])\n","\n","# # Create a Voting Classifier\n","# voting_clf = VotingClassifier(\n","#     estimators=[\n","#         ('log_reg', log_reg),\n","#         ('rf', rf),\n","#         ('svc', svc),\n","#         ('et', et)\n","#     ],\n","#     voting='hard'  # Use 'soft' for probability averaging or 'hard' for majority voting\n","# )\n","\n","# # Train and evaluate the Voting Classifier\n","# def evaluate_ensemble(X_train, X_test, y_train, y_test, models, metrics):\n","#     results = {}\n","#     for name, model in models.items():\n","#         model.fit(X_train, y_train)\n","#         y_pred = model.predict(X_test)\n","#         results[name] = {metric_name: metric(y_test, y_pred) for metric_name, metric in metrics.items()}\n","#     return results\n","\n","# # Example usage of the function\n","# ensemble_results = evaluate_ensemble(\n","#     X_train_encoded_and_scaled, X_test_encoded_and_scaled, y_train, y_test,\n","#     {'Voting Classifier': voting_clf}, metrics\n","# )\n","\n","# # Print ensemble results\n","# print(json.dumps(ensemble_results, indent=4))"],"metadata":{"id":"4qUm78zF_xzw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving"],"metadata":{"id":"4e56KVp79hW1"}},{"cell_type":"code","source":["# Save the model\n","with open(\"model.pkl\", \"wb\") as f:\n","    pkl.dump(log_reg_clf, f)"],"metadata":{"id":"Yzw_aU58DGQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save standard scaler\n","with open('scaler.pkl', 'wb') as f:\n","    pkl.dump(scaler, f)"],"metadata":{"id":"Y9rOCmbo98aU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save OHE\n","with open('encoder.pkl', 'wb') as f:\n","    pkl.dump(ohe, f)"],"metadata":{"id":"cnZfNEZ09rab"},"execution_count":null,"outputs":[]}]}