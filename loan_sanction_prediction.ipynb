{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1724139956220},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1724054788079},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723962163468},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723873090109},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723778725460},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723720997842},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723702539126},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723629465004},{"file_id":"1IG3rtuSryUSW9aJSzSv3Cj9Q3sjlK-hV","timestamp":1723606680546},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723550355534},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723525668579}],"collapsed_sections":["fcYXDXxxbYzn","hrPFXTgf0z2m","gWXVdU3rhoRK","UUPAWn7ktONj","I1DWjlExrRyn","TVgjxyQz01mx","TvlrWpey4jBW","xx72OX6cJeef","4YyR3UMbJlEt","NvBgnlC3Mgb9","1iHpN_dsu6ob","DuiIV2QMQJMw","kF98C-5GwPWy","7BMKGNEuwXy0","5w2ZGBx5HBXD","MLw8NXA0IIQV","A-y-VXdyIEjj","M1KAAAvL4ht-","3DtS8s2631xQ","Ul3c15DmIfFe","ATpMLVJjIsrt","Lb9fJULYJW2U","tOkesqzLJQye","IubF6rcAJa9i","4e56KVp79hW1"],"authorship_tag":"ABX9TyOoQcqOcf4TtTaEcy57elPn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Configuring libraries & utilities"],"metadata":{"id":"6U4S3d7fsRLq"}},{"cell_type":"code","source":["# Upgrade scikit-learn\n","!pip install --upgrade scikit-learn -q\n","\n","# If you are using Google Colab, after upgrading scikit-learn, you may need to restart the runtime to run the following cells."],"metadata":{"id":"LAQIHv6GgIlF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standard Library Imports\n","import os\n","import json\n","import math\n","\n","# Google Drive Integration\n","from google.colab import drive\n","\n","# Data Manipulation and Preprocessing\n","import numpy as np\n","import pandas as pd\n","\n","# Data Visualization\n","import seaborn as sns\n","import scipy.stats as stats\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","\n","# Data Preprocessing and Transformation\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from imblearn.over_sampling import SMOTE\n","from sklearn.compose import ColumnTransformer\n","from imblearn.pipeline import Pipeline as ImbalancedPipeline\n","from sklearn.preprocessing import (\n","    LabelEncoder, OneHotEncoder, StandardScaler, PowerTransformer, QuantileTransformer,\n","    FunctionTransformer\n",")\n","\n","# Machine Learning Models\n","from sklearn.svm import SVC\n","from xgboost import XGBClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n","    VotingClassifier\n",")\n","\n","# Model Evaluation and Hyperparameter Tuning\n","from sklearn.model_selection import cross_val_score, GridSearchCV\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Saving\n","import joblib\n","\n","# Custom Modules for additional functionalities\n","try:\n","    import flash as fz\n","except ImportError:\n","    # Install the custom module if not present\n","    !pip install git+https://github.com/Althaf9900/flash.git -q\n","    import flash as fz"],"metadata":{"id":"I1_atIHsa4Br"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive if it's not already mounted\n","mount_point = \"/content/drive\"\n","\n","if not os.path.ismount(mount_point):\n","    print(\"Mounting Google Drive...\")\n","    drive.mount(mount_point)\n","else:\n","    print(\"Google Drive is already mounted.\")\n","\n","%cd /content/drive/MyDrive/Projects/loan-sanction-prediction"],"metadata":{"collapsed":true,"id":"WRozjl381lRa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Initial data assessment & preparation\n"],"metadata":{"id":"fcYXDXxxbYzn"}},{"cell_type":"code","source":["# Loading the train dataset\n","df_copy = pd.read_csv('loan_sanction_train.csv')\n","df = df_copy"],"metadata":{"id":"GaiVFZ039qYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Understanding structure of the dataset\n","df.sample(5)"],"metadata":{"id":"--gXs5jR_W5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking whether Loan_ID contains duplicate IDs\n","df['Loan_ID'].duplicated().sum()"],"metadata":{"id":"eAjHUDF4VQ3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking whether the dataset is imbalanced or not\n","plt.pie(df['Loan_Status'].value_counts(), labels = df['Loan_Status'].unique(), autopct='%0.2f%%',\n","        shadow=True, explode=(0, 0.1), counterclock=False, colors=['lime', 'cyan'])\n","plt.show()"],"metadata":{"id":"ry1walf3AM55"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Columns of the dataset\n","print(df.columns)"],"metadata":{"id":"6GjPVOWM2il5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting some information about the dataset\n","df.info()"],"metadata":{"id":"lUPbGCTr_b20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dropping useless features\n","df.drop('Loan_ID', axis=1, inplace=True)"],"metadata":{"id":"G2JG-P6ldwgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting numerical features & categorical features from the dataset using a custom made module\n","num_cols = fz.get_num_col(df)\n","cat_cols = fz.get_cat_col(df, ignore_cols=['Loan_Status'])\n","\n","# Number of categorical features\n","n_cat_cols = len(cat_cols)\n","n_num_cols = len(num_cols)\n","\n","\n","print(num_cols)\n","print(cat_cols)\n","\n","print(f'Number of numerical features: {n_num_cols}')\n","print(f'Number of categorical features: {n_cat_cols}')"],"metadata":{"id":"8wFiCJgV_eIx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (Before data cleaning)\n"],"metadata":{"id":"hrPFXTgf0z2m"}},{"cell_type":"markdown","source":["\n","## Outlier analysis on numerical features\n"],"metadata":{"id":"K6aKkPNA24wd"}},{"cell_type":"code","source":["# Statistical measures\n","df[num_cols].describe().T"],"metadata":{"id":"TB8kLnYq4TRX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histogram & Box-plot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 10))\n","\n","# Plotting histograms and boxplots\n","for i, col in enumerate(num_cols):\n","    # Histogram\n","    sns.histplot(df[col], kde=True, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Histogram of {col}')\n","\n","    # Boxplot\n","    sns.boxplot(data=df, x=col, ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Boxplot of {col}')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"CmE1HHsT4v0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize a list to store features with outliers\n","ftrs_with_outliers = []\n","\n","# Iterate through each numerical features to calculate IQR and identify outliers\n","for feature in df[num_cols].columns:\n","    # Calculate Q1, Q3, and IQR\n","    Q1 = df[feature].quantile(0.25)\n","    Q3 = df[feature].quantile(0.75)\n","    IQR = Q3 - Q1\n","\n","    # Define outlier bounds\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","\n","    # Check if the feature contains any outliers\n","    if ((df[feature] < lower_bound) | (df[feature] > upper_bound)).any():\n","        ftrs_with_outliers.append(feature)\n","\n","print(\"Features with outliers:\", ftrs_with_outliers)"],"metadata":{"id":"BgqOq69_VSzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applicant income\n","Q1 = df['ApplicantIncome'].quantile(0.25)\n","Q3 = df['ApplicantIncome'].quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","df[(df['ApplicantIncome'] < lower_bound) | (df['ApplicantIncome'] > upper_bound)]['ApplicantIncome'].sort_values()"],"metadata":{"id":"tqfslX_KW2Hw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Co-applicant income\n","Q1 = df['CoapplicantIncome'].quantile(0.25)\n","Q3 = df['CoapplicantIncome'].quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","df[(df['CoapplicantIncome'] < lower_bound) | (df['CoapplicantIncome'] > upper_bound)]['CoapplicantIncome'].sort_values()"],"metadata":{"id":"cnpLUQyqFVdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loan amount\n","Q1 = df['LoanAmount'].quantile(0.25)\n","Q3 = df['LoanAmount'].quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","df[(df['LoanAmount'] < lower_bound) | (df['LoanAmount'] > upper_bound)]['LoanAmount'].sort_values()"],"metadata":{"id":"NQlfiS8zFg4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- There are many outliers on the upper side of all numerical features.\n","\n","- None of the numerical features have outliers on the lower side.\n","\n","- Since we only have few data points, we can't afford to drop any data points.\n","    \n","- None of the numerical features follow a normal distribution.\n","\n","- The outliers appear to be valid and are not due to data entry issues.\n","\n","- Since the outliers are valid, apply capping methods, such as:\n","\n","    - Custom threshold capping: Set a threshold value based on analysis of the boxplots.\n","    - Percentile-based capping: Limit outliers to a specified percentile range.\n","    - Median imputation: Replace extreme values with the median.\n","\n","- After building the predictive model, evaluate the accuracy of all capping methods."],"metadata":{"id":"Nxw1gMdUdRoH"}},{"cell_type":"markdown","source":["\n","## Missing value analysis\n"],"metadata":{"id":"gWXVdU3rhoRK"}},{"cell_type":"code","source":["# Calculate the percentage of missing values in numerical features\n","num_miss_pct = df[num_cols].isna().mean()*100\n","\n","# Filter out categorical features with no missing values\n","num_miss_pct = num_miss_pct[num_miss_pct > 0]\n","\n","# Print the percentage of missing values for each numerical features with missing values\n","for index, value in num_miss_pct.items():\n","    print(f\"{index}: {round(value, 2)}%\")\n","\n","# List of numerical features that have missing values\n","num_cols_with_na = num_miss_pct.index.to_list()\n","\n","print(num_cols_with_na)"],"metadata":{"id":"A7Db6Hti_wi_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the percentage of missing values in categorical features\n","cat_miss_pct = df[cat_cols].isna().mean() * 100\n","\n","# Filter out categorical features with no missing values\n","cat_miss_pct = cat_miss_pct[cat_miss_pct > 0]\n","\n","# Print the percentage of missing values for each categorical feature with missing values\n","for feature, pct in cat_miss_pct.items():\n","    print(f\"{feature}: {round(pct, 2)}%\")\n","\n","# List of categorical features that have missing values\n","cat_cols_with_na = cat_miss_pct.index.to_list()\n","\n","print(cat_cols_with_na)"],"metadata":{"id":"bYOfY62oceGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing whether the missing values are missing at random or not\n","plt.figure(figsize=(15, 4))\n","sns.heatmap(df.isna(), cbar=False, cmap=\"Blues\", yticklabels=False)\n","plt.xticks(rotation=45)\n","plt.show()"],"metadata":{"id":"buU_pHC2WwQg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting histogram of numerical features that have missing values to decide whether to use mean or median\n","sns.histplot(df['LoanAmount'], kde=True)\n","plt.show()"],"metadata":{"id":"eWwW3tdPcD1c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- Only one numerical feature (['LoanAmount']) has missing values.\n","\n","- Six categorical features (['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History']) have missing values.\n","\n","- Since we only have few data points, we cannot afford to drop any data points.\n","\n","- The percentage of missing values is low across all features, so there is no need to drop any columns.\n","\n","- It appears that the missingness of values is random.\n","\n","- Missing value handling:\n","\n","    - Median imputation (for numerical features that are not normally distributed):\n","        - Loan amount\n","\n","    - Mode imputation:\n","        - Categorical features\n"],"metadata":{"id":"o0G6gChoKxfU"}},{"cell_type":"markdown","source":["\n","# Data Cleaning\n"],"metadata":{"id":"UUPAWn7ktONj"}},{"cell_type":"markdown","source":["Data cleaning conclusions:\n","\n","- Outlier handling:\n","\n","    - Since the outliers are valid, apply capping methods, such as:\n","\n","        - Custom threshold capping: Set a threshold value based on analysis of the boxplots.\n","        - Percentile-based capping: Limit outliers to a specified percentile range.\n","        - Median imputation: Replace outliers with the median.\n","\n","\n","- Missing value handling:\n","\n","    - Median imputation (for numerical features that are not normally distributed):\n","        - Loan amount\n","\n","    - Mode imputation:\n","        - Categorical features\n","\n","- Data type adjustments:\n","\n","    - Data type comapatibility:\n","        - Applicant income: float\n","        - Loan_Amount_Term: int then, str\n","        - Credit_History: int then, str\n","\n","    - Memory usage optimization:\n","        - Categorical features: category\n","        - Numerical features: float64 -> float32"],"metadata":{"id":"rsFPXBYzVz27"}},{"cell_type":"code","source":["# Outlier handling: Custom threshold capping\n","\n","# Define the cap value\n","cap_values = [20833, 8980, 500]\n","\n","for i, feature in enumerate(ftrs_with_outliers):\n","    # Cap the values\n","    df[feature] = df[feature].clip(upper=cap_values[i])"],"metadata":{"id":"arnnaMYTD-UE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Missing value handling\n","\n","# Imputing missing values in LoanAmount feature with median\n","median_imputer = SimpleImputer(strategy='median')\n","df['LoanAmount'] = median_imputer.fit_transform(df[['LoanAmount']])\n","\n","# Imputing missing values in categorical features with mode\n","mode_imputer = SimpleImputer(strategy='most_frequent')\n","df[cat_cols_with_na] = mode_imputer.fit_transform(df[cat_cols_with_na])\n","\n","# Test\n","if df.isna().sum().sum() == 0:\n","    print(\"There are no missing values left in the DataFrame.\")\n","else:\n","    print(\"There are still missing values in the DataFrame.\")"],"metadata":{"id":"0INL_0mEih5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking memory usage before dtype adjustments\n","print(\"Memory usage before adjustment:\", df.memory_usage(deep=True).sum())\n","print()\n","\n","# Data type adjustments\n","\n","# Data type compatibility\n","df['ApplicantIncome'] = df['ApplicantIncome'].astype(float)\n","\n","# Converting numerical categorical features to int\n","df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype(int)\n","df['Credit_History'] = df['Credit_History'].astype(int)\n","\n","# Converting numerical categorical features to str\n","df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype(str)\n","df['Credit_History'] = df['Credit_History'].astype(str)\n","\n","# Memory usage optimization\n","df[cat_cols] = df[cat_cols].astype('category')\n","\n","# Print data types to confirm changes\n","print(df.dtypes)\n","\n","# Checking memory usage after dtype adjustments\n","print()\n","print(\"Memory usage after adjustment:\", df.memory_usage(deep=True).sum())"],"metadata":{"id":"kqpDRtGPlVJo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (On Independent features)\n"],"metadata":{"id":"I1DWjlExrRyn"}},{"cell_type":"markdown","source":["\n","## Univariate analysis\n"],"metadata":{"id":"TVgjxyQz01mx"}},{"cell_type":"markdown","source":["\n","### Numerical\n"],"metadata":{"id":"NgEA39dklZ8N"}},{"cell_type":"code","source":["df[num_cols].describe().T"],"metadata":{"id":"mDi3hcLNlfpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Skewness and Kurtosis\n","skew_kurt_dict = {}\n","for col in num_cols:\n","    skew_kurt_dict[col] = {\n","        'skewness': round(float(df[col].skew()), 2),\n","        'kurtosis': round(float(df[col].kurtosis()), 2)\n","    }\n","\n","print(json.dumps(skew_kurt_dict, indent=4))"],"metadata":{"id":"QOOWv8wfp3Px"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histogram & Box-plot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 10))\n","\n","# Plotting histograms and boxplots\n","for i, col in enumerate(num_cols):\n","    # Histogram\n","    sns.histplot(df[col], kde=True, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Histogram of {col}')\n","\n","    # Boxplot\n","    sns.boxplot(data=df, x=col, ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Boxplot of {col}')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"B4nhzoKGmhYz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- Applicant income and loan amount approximately follow a log-normal distribution.\n","- Feature transformation is needed for all numerical features.\n","- It appears that individuals with a co-applicant income of 0 do not have a co-applicant. Therefore, create a feature named 'Has_coapplicant'. In this feature, set individuals with a co-applicant income of 0 to 'No, and those with a non-zero co-applicant income to 'Yes."],"metadata":{"id":"-A0YZbXsnK3C"}},{"cell_type":"markdown","source":["\n","### Categorical\n"],"metadata":{"id":"cRUOZJ-l2h9s"}},{"cell_type":"code","source":["# Statistical measures\n","df[cat_cols].describe().T"],"metadata":{"id":"LkOlLNwQ32Q_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Countplot\n","\n","# Calculate number of rows and columns needed for subplots\n","n_cols = 3\n","n_rows = math.ceil(n_cat_cols / n_cols)\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n","\n","# Flatten axs array if it's multidimensional\n","axs = axs.flatten()\n","\n","# Plot countplots and set titles\n","for i, feature in enumerate(cat_cols):\n","    sns.countplot(data=df, x=feature, ax=axs[i])\n","    axs[i].set_title(feature)\n","    axs[i].tick_params(axis='x', rotation=45)\n","\n","# Turn off any unused subplots\n","for j in range(len(cat_cols), len(axs)):\n","    axs[j].axis('off')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"86PJgdX_3uoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- Males take out more loans compared to females.\n","- Married individuals take out more loans compared to - unmarried individuals.\n","- People without dependents take out more loans compared to those with dependents.\n","- Graduates take out more loans compared to non-graduates.\n","- Non-self-employed individuals take out more loans compared to self-employed individuals.\n","- Most people opt for a loan term of 360 months (30 years), followed by 180 months (15 years).\n","- People with a credit history of 1 take out more loans compared to those with a credit history of 0.\n","- People living in semi-urban areas take out more loans compared to those living in rural and urban areas. Rural residents take out the fewest loans. Although these relationships aren't strong, they may represent general trends."],"metadata":{"id":"Z1pkXN3GhmYk"}},{"cell_type":"markdown","source":["\n","## Bivariate analysis\n"],"metadata":{"id":"TvlrWpey4jBW"}},{"cell_type":"markdown","source":["\n","### Numerical - Numerical\n"],"metadata":{"id":"xx72OX6cJeef"}},{"cell_type":"code","source":["# Pairplot\n","sns.pairplot(df[num_cols], diag_kind='kde')\n","plt.show()"],"metadata":{"id":"82F8jFg-8xI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# regplot with PairGrid\n","graph = sns.PairGrid(df)\n","\n","# type of graph for diagonal\n","graph = graph.map_diag(sns.histplot, kde=True)\n","\n","# type of graph for non-diagonal\n","graph = graph.map_offdiag(sns.regplot, scatter=True, line_kws={'color': 'red'})\n","\n","plt.show()"],"metadata":{"id":"B8ShB9vZA3tU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a custom colorbar\n","# Define custom colors\n","colors = [\"#FF0000\", \"#FFFF00\", \"#00FF00\"]  # Red to Yellow to Green\n","\n","# Create a custom colormap\n","custom_cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", colors)"],"metadata":{"id":"xiZWa00gqpn8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to create a mask for the upper triangle\n","def create_mask(df, method):\n","    corr = df.corr(method=method)\n","    mask = np.triu(np.ones_like(corr, dtype=bool))\n","    np.fill_diagonal(mask, False)  # Optional: keep or remove diagonal elements\n","    return mask\n","\n","# Function to plot heatmap\n","def plot_heatmap(df, method, ax, cmap, title):\n","    mask = create_mask(df, method)\n","    sns.heatmap(df.corr(method=method), mask=mask, annot=True, cmap=cmap, ax=ax, cbar=False)\n","    ax.set_title(title)"],"metadata":{"id":"QS8prjqVxyGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(1, 3, figsize=(13, 5), gridspec_kw={'width_ratios': [1, 1, 0.05]})\n","\n","# Plot Pearson and Spearman heatmaps\n","plot_heatmap(df[num_cols], 'pearson', axs[0], custom_cmap, 'Pearson Correlation Heatmap')\n","plot_heatmap(df[num_cols], 'spearman', axs[1], custom_cmap, 'Spearman Correlation Heatmap')\n","\n","# Create a common colorbar for both heatmaps\n","cbar = fig.colorbar(axs[0].collections[0], cax=axs[2])\n","\n","# Adjust layout to prevent overlapping\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"WySczazSJ0AZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- None of the features show a strong linear relationship with each other. However, there is a moderate relationship between applicant income and loan amount. This makes sense because individuals with higher incomes often need larger loan amounts.\n","\n","- Both Pearson and Spearman correlation coefficients show similar patterns, but their values are slightly different. Since the heatmaps from both are similar, the exact values are less important. In this case, Spearman's correlation is more suitable because the data isn't normally distributed, doesn't have a linear relationship between features, and has outliers."],"metadata":{"id":"e87zhIkG2MKg"}},{"cell_type":"markdown","source":["\n","### Categorical - Categorical\n"],"metadata":{"id":"4YyR3UMbJlEt"}},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols * (n_cat_cols - 1) // 2, 2, figsize=(13, 140))  # Adjust size as needed\n","axs = axs.reshape(-1, 2)  # Flatten the array of subplots\n","\n","# Plot heatmaps\n","plot_index = 0\n","for i in range(n_cat_cols):\n","    for j in range(i + 1, n_cat_cols):\n","        # Create contingency tables\n","        contingency_table_1 = pd.crosstab(df[cat_cols[i]], df[cat_cols[j]], normalize='index') * 100\n","        contingency_table_2 = pd.crosstab(df[cat_cols[i]], df[cat_cols[j]], normalize='columns') * 100\n","\n","        # Plot heatmaps\n","        sns.heatmap(contingency_table_1, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                    xticklabels=True, yticklabels=True, ax=axs[plot_index, 0])\n","        sns.heatmap(contingency_table_2, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                    xticklabels=True, yticklabels=True, ax=axs[plot_index, 1])\n","\n","        # Set titles for each subplot\n","        axs[plot_index, 0].set_title(f\"{cat_cols[i]} vs {cat_cols[j]} (Index Normalized)\")\n","        axs[plot_index, 1].set_title(f\"{cat_cols[i]} vs {cat_cols[j]} (Column Normalized)\")\n","\n","        plot_index += 1\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"64_F3iSAsNP1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Numerical - Categorical\n"],"metadata":{"id":"NvBgnlC3Mgb9"}},{"cell_type":"code","source":["# Applicant income\n","num_col = 'ApplicantIncome'\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(15, n_cat_cols * 4))\n","\n","for i in range(len(cat_cols)):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x=cat_cols[i], y=num_col, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_col} by {cat_cols[i]}')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_col} by {cat_cols[i]}')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"T3Ri0qCZ7S9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Coapplicant income\n","num_col = 'CoapplicantIncome'\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(15, n_cat_cols * 4))\n","\n","for i in range(len(cat_cols)):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x=cat_cols[i], y=num_col, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_col} by {cat_cols[i]}')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_col} by {cat_cols[i]}')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"j_PPBJqN9O_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loan amount\n","num_col = 'LoanAmount'\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(15, n_cat_cols * 4))\n","\n","for i in range(len(cat_cols)):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x=cat_cols[i], y=num_col, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_col} by {cat_cols[i]}')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x=cat_cols[i], y=num_col, errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_col} by {cat_cols[i]}')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"gyNZz0No9SFq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Feature construction\n"],"metadata":{"id":"1iHpN_dsu6ob"}},{"cell_type":"markdown","source":["- It appears that individuals with a co-applicant income of 0 do not have a co-applicant. Therefore, create a feature named 'Has_coapplicant'. In this feature, set individuals with a co-applicant income of 0 to 'No', and those with a non-zero co-applicant income to 'Yes'."],"metadata":{"id":"DilMLweTueyk"}},{"cell_type":"code","source":["df['Has_coapplicant'] = np.where(df['CoapplicantIncome'] == 0, 'No', 'Yes')\n","df['Has_coapplicant']"],"metadata":{"id":"Op8pR4w8kIr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Appending newly constructed features to feature lists based on their type\n","cat_cols.append('Has_coapplicant')"],"metadata":{"id":"EslfarRnJkzN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (On newly constructed features)\n"],"metadata":{"id":"DuiIV2QMQJMw"}},{"cell_type":"markdown","source":["\n","## Univariate analysis\n"],"metadata":{"id":"kF98C-5GwPWy"}},{"cell_type":"code","source":["df['Has_coapplicant'].describe().T"],"metadata":{"id":"TLhb427hwUCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.countplot(x = df['Has_coapplicant'])\n","plt.show()"],"metadata":{"id":"dAiY4fGRwXHK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Bivariate analysis\n"],"metadata":{"id":"7BMKGNEuwXy0"}},{"cell_type":"markdown","source":["\n","#### Categorical - Categorical"],"metadata":{"id":"pQteagYGy5b_"}},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(13, 40))  # Adjust size as needed\n","axs = axs.reshape(-1, 2)  # Flatten the array of subplots\n","\n","# Plot heatmaps\n","for i in range(n_cat_cols):\n","    # Create contingency tables\n","    contingency_table_1 = pd.crosstab(df[cat_cols[i]], df['Has_coapplicant'], normalize='index') * 100\n","    contingency_table_2 = pd.crosstab(df[cat_cols[i]], df['Has_coapplicant'], normalize='columns') * 100\n","\n","    # Plot heatmaps\n","    sns.heatmap(contingency_table_1, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 0])\n","    sns.heatmap(contingency_table_2, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 1])\n","\n","    # Set titles for each subplot\n","    axs[i, 0].set_title(f\"{cat_cols[i]} vs Has_coapplicant (Index Normalized)\")\n","    axs[i, 1].set_title(f\"{cat_cols[i]} vs Has_coapplicant (Column Normalized)\")\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"pIGdHXidlZEw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### Numerical - Categorical\n"],"metadata":{"id":"LJ7rmAjWzHzZ"}},{"cell_type":"code","source":["# Violinplot & Pointplot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 9))\n","\n","for i in range(n_num_cols):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x='Has_coapplicant', y=num_cols[i], ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_cols[i]} by Has_coapplicant')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x='Has_coapplicant', y=num_cols[i], errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x='Has_coapplicant', y=num_cols[i], errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_cols[i]} by Has_coapplicant')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TUk79SjZy3oM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (Between independent features and target feature)"],"metadata":{"id":"5w2ZGBx5HBXD"}},{"cell_type":"markdown","source":["\n","## Categorical - Categorical\n"],"metadata":{"id":"MLw8NXA0IIQV"}},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(13, 40))  # Adjust size as needed\n","axs = axs.reshape(-1, 2)  # Flatten the array of subplots\n","\n","# Plot heatmaps\n","for i in range(n_cat_cols):\n","    # Create contingency tables\n","    contingency_table_1 = pd.crosstab(df[cat_cols[i]], df['Loan_Status'], normalize='index') * 100\n","    contingency_table_2 = pd.crosstab(df[cat_cols[i]], df['Loan_Status'], normalize='columns') * 100\n","\n","    # Plot heatmaps\n","    sns.heatmap(contingency_table_1, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 0])\n","    sns.heatmap(contingency_table_2, annot=True, cmap=custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 1])\n","\n","    # Set titles for each subplot\n","    axs[i, 0].set_title(f\"{cat_cols[i]} vs Loan_Status (Index Normalized)\")\n","    axs[i, 1].set_title(f\"{cat_cols[i]} vs Loan_Status (Column Normalized)\")\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"WgbeDDifHiQ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Numerical - Categorical\n"],"metadata":{"id":"A-y-VXdyIEjj"}},{"cell_type":"code","source":["# Violinplot & Pointplot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 9))\n","\n","for i in range(n_num_cols):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x='Loan_Status', y=num_cols[i], ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_cols[i]} by Loan_Status')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x='Loan_Status', y=num_cols[i], errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x='Loan_Status', y=num_cols[i], errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_cols[i]} by Loan_Status')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"L-a5UIaJHp-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Feature transformation\n"],"metadata":{"id":"M1KAAAvL4ht-"}},{"cell_type":"code","source":["# Epsilon to avoid log(0) and sqrt(0)\n","epsilon = 1e-10\n","\n","# Initialize transformers\n","transformers = {\n","    'Log': FunctionTransformer(func=lambda X: np.log(X + epsilon), validate=False),\n","    'Square Root': FunctionTransformer(func=lambda X: np.sqrt(X + epsilon),\n","                                       validate=False),\n","    'Square': FunctionTransformer(func=np.square, validate=False),\n","    'Reciprocal': FunctionTransformer(func=lambda X: np.reciprocal(X + epsilon),\n","                                      validate=False),\n","    'Yeo-Johnson': PowerTransformer(standardize=False),\n","    'Quantile': QuantileTransformer(n_quantiles=df.shape[0], output_distribution='normal')\n","}\n","\n","# Apply transformations\n","transformed_data = {}\n","for name, transformer in transformers.items():\n","    transformed_data[name] = pd.DataFrame(transformer.fit_transform(df[num_cols]),\n","                                          columns=num_cols)"],"metadata":{"id":"SAregm_b1l0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histogram\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, len(transformers) + 1, figsize=(26, 9))\n","\n","# Plot histograms\n","for i, col in enumerate(num_cols):\n","    # Original\n","    sns.histplot(df[col], kde=True, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Original {col}')\n","\n","    for j, (name, transformed_df) in enumerate(transformed_data.items()):\n","        sns.histplot(transformed_df[col], kde=True, ax=axs[i, j + 1])\n","        axs[i, j + 1].set_title(f'{name} {col}')\n","\n","# Turn off y axis labels for all subplots\n","axs = axs.flatten()\n","for ax in axs:\n","    ax.set_ylabel('')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"gmF2YBK-zsNW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# QQ plot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, len(transformers) + 1, figsize=(26, 9))\n","\n","# Plot Q-Q plots\n","for i, col in enumerate(num_cols):\n","    # Original\n","    stats.probplot(df[col], dist=\"norm\", plot=axs[i, 0])\n","    axs[i, 0].set_title(f'Original {col}')\n","    axs[i, 0].get_lines()[1].set_color('red')  # Make the reference line red\n","\n","    for j, (name, transformed_df) in enumerate(transformed_data.items()):\n","        stats.probplot(transformed_df[col], dist=\"norm\", plot=axs[i, j + 1])\n","        axs[i, j + 1].set_title(f'{name} {col}')\n","        axs[i, j + 1].get_lines()[1].set_color('red')  # Make the reference line red\n","\n","# Turn off x and y axis labels for all subplots\n","axs = axs.flatten()\n","for ax in axs:\n","    ax.set_xlabel('')\n","    ax.set_ylabel('')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"dm5_EpPm1_9M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- ApplicantIncome & LoanAmount: Quantile Transform\n","- CoapplicantIncome: Reciprocal Transform"],"metadata":{"id":"vK6XEpnv-ADI"}},{"cell_type":"code","source":["df['ApplicantIncome'] = transformed_data['Quantile']['ApplicantIncome']\n","df['CoapplicantIncome'] = transformed_data['Reciprocal']['CoapplicantIncome']\n","df['LoanAmount'] = transformed_data['Quantile']['LoanAmount']"],"metadata":{"id":"CrtowETC-sem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Building predictive model"],"metadata":{"id":"4YRoJuM-0Lu7"}},{"cell_type":"markdown","source":["\n","## Preparing the data\n"],"metadata":{"id":"3DtS8s2631xQ"}},{"cell_type":"code","source":["# Splitting the data into features and target\n","X_train = df.drop('Loan_Status', axis=1)\n","y_train = df['Loan_Status']"],"metadata":{"id":"cWfE79OQpVSU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Label encode the target feature\n","\n","# Initialize the LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit and transform the target feature\n","y_train = le.fit_transform(y_train)"],"metadata":{"id":"RcbjjeDIjRfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Column transformer for preprocessing\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('scaler', StandardScaler(), num_cols),  # Scaling numerical columns\n","        ('encoder', OneHotEncoder(drop='first', sparse_output=False), cat_cols) # OneHotEncoding categorical columns\n","    ],\n","    remainder='passthrough'  # Keep the columns not listed in num_cols or cat_cols as is\n",")"],"metadata":{"id":"PtX68o5CEp5O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pipeline for training data\n","pipe_train = ImbalancedPipeline([\n","    ('preprocessor', preprocessor),\n","    ('imbalance', SMOTE(random_state=42))\n","])"],"metadata":{"id":"9p0MyzCC_tCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pipeline for testing data\n","pipe_test = Pipeline([\n","    ('preprocessor', preprocessor)\n","])"],"metadata":{"id":"ghaRUJN_g0wd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visual output of the pipeline\n","pipe_train.fit(X_train, y_train)"],"metadata":{"id":"B38v2A7ulB4P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing and Transforming training data using pipeline\n","X_train_transformed, y_train = pipe_train.fit_resample(X_train, y_train)"],"metadata":{"id":"I5r6wuybIz6p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Model selection (Before hyperparameter tuning)\n"],"metadata":{"id":"tOkesqzLJQye"}},{"cell_type":"code","source":["# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression,\n","    'Random Forest': RandomForestClassifier,\n","    'Gradient Boosting': GradientBoostingClassifier,\n","    'Support Vector Machine': SVC,\n","    'KNN': KNeighborsClassifier,\n","    'Decision Trees': DecisionTreeClassifier,\n","    'Xgboost': XGBClassifier,\n","    'Extra Trees': ExtraTreesClassifier\n","}\n","# Define metric functions\n","metrics = {\n","    'accuracy': accuracy_score,\n","    'precision': precision_score,\n","    'recall': recall_score,\n","    'f1': f1_score\n","}"],"metadata":{"id":"MQgmPbGs9Qwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval_models_across_metrics(models, metrics, X_train, y_train, cv=5, sort=False, model_params=None):\n","\n","    models_across_metrics = {metric: {} for metric in metrics}\n","\n","    for metric in metrics:\n","        for model_name, model in models.items():\n","\n","            if isinstance(model_params, dict):\n","                cv_scores = cross_val_score(model(**model_params[model_name]), X_train,\n","                                            y_train, cv=cv, scoring=metric)\n","            else:\n","                cv_scores = cross_val_score(model(), X_train,\n","                                            y_train, cv=cv, scoring=metric)\n","\n","            cv_scores_mean = cv_scores.mean()\n","\n","            models_across_metrics[metric][model_name] = round(cv_scores_mean, 3)\n","\n","    if sort:\n","        for metric, model_scores in models_across_metrics.items():\n","            models_across_metrics[metric] = dict(\n","                sorted(model_scores.items(), key=lambda item: item[1], reverse=True)\n","                )\n","\n","    return models_across_metrics"],"metadata":{"id":"NCXNRkQqoQcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Comparing models across metrics\n","models_across_metrics = eval_models_across_metrics(models, metrics.keys(), X_train_transformed,\n","                                                   y_train, sort=True)\n","print(json.dumps(models_across_metrics, indent=4))"],"metadata":{"id":"ARzQsfocvwGQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","After evaluating the metrics, I have decided to focus on the top 3 models: Random Forest Classifier, Extra Trees Classifier, Xgboost Classifier. These models have demonstrated strong performance across the different metrics, making them the best candidates for further fine-tuning and optimization."],"metadata":{"id":"9_21ZbTWH8dq"}},{"cell_type":"markdown","source":["\n","## Hyperparameter tuning\n"],"metadata":{"id":"IubF6rcAJa9i"}},{"cell_type":"code","source":["# Define top models for further hyperparameter tuning\n","models = {\n","    'Random Forest': RandomForestClassifier,\n","    'Xgboost': XGBClassifier,\n","    'Extra Trees': ExtraTreesClassifier\n","}"],"metadata":{"id":"AfG2BpNjKF4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define parameter grids\n","param_grids = {\n","    'Random Forest': {\n","        'n_estimators': [50, 100, 200],\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4]\n","    },\n","    'Xgboost': {\n","        'n_estimators': [50, 100, 200],\n","        'max_depth': [3, 6, 10],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'subsample': [0.8, 0.9, 1.0],\n","        'colsample_bytree': [0.8, 0.9, 1.0],\n","        'gamma': [0, 0.1, 0.2]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [50, 100, 200],\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'bootstrap': [True, False]\n","    }\n","}"],"metadata":{"id":"nv1r5gjDTo_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def perform_grid_search(models, param_grids, X_train, y_train):\n","    best_params = {}\n","    for model_name, model in models.items():\n","        print(f\"Processing {model_name}...\")\n","        param_grid = param_grids[model_name]\n","        grid_search = GridSearchCV(estimator=model(), param_grid=param_grid,\n","                                   scoring='accuracy', cv=5, n_jobs=-1)\n","        grid_search.fit(X_train, y_train)\n","        best_params[model_name] = {\n","            'Best Parameters': grid_search.best_params_,\n","            'Average accuracy score on the best parameters': round(grid_search.best_score_, 3)\n","        }\n","    return best_params"],"metadata":{"id":"yvIm51meOyWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finding best hyperparameters on top models using GridSearchCV\n","best_params = perform_grid_search(models, param_grids, X_train_transformed, y_train)\n","\n","# Print best results from grid search\n","print(json.dumps(best_params, indent=4))"],"metadata":{"id":"uyYCLkcQpMrP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_params_manual = {\n","    'Random Forest': {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1},\n","    'Xgboost': {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 0},\n","    'Extra Trees': {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}\n","}"],"metadata":{"id":"oQQr2C5GE9Q0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Comparing top models across metrics after hyperparameter tuning\n","models_across_metrics = eval_models_across_metrics(models, metrics.keys(), X_train_transformed,\n","                                                   y_train, sort=True, model_params=best_params_manual)\n","print(json.dumps(models_across_metrics, indent=4))"],"metadata":{"id":"-YCeVOVMmSW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["estimators = []\n","for model_name, model in models.items():\n","    estimators.append((model_name, model(**best_params_manual[model_name])))"],"metadata":{"id":"j2L3a2xVq7U3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a voting classifier (hard voting)\n","voting_clf_hard = VotingClassifier(estimators=estimators, voting='hard')\n","\n","# Create a voting classifier (soft voting)\n","voting_clf_soft = VotingClassifier(estimators=estimators, voting='soft')\n","\n","# Apply cross-validation\n","cv_scores_h = cross_val_score(voting_clf_hard, X_train_transformed, y_train, cv=5,\n","                              scoring='accuracy')\n","cv_scores_s = cross_val_score(voting_clf_soft, X_train_transformed, y_train, cv=5,\n","                              scoring='accuracy')\n","\n","# Print cross-validation results\n","print(f'Mean Cross-Validation Accuracy on Hard Margin: {cv_scores_h.mean():.3f}')\n","print(f'Mean Cross-Validation Accuracy on Soft Margin: {cv_scores_s.mean():.3f}')"],"metadata":{"id":"mCVRtfc_pPcS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving"],"metadata":{"id":"4e56KVp79hW1"}},{"cell_type":"code","source":["# Save the Machine Learning model\n","joblib.dump(voting_clf_soft, 'model.joblib')"],"metadata":{"id":"Yzw_aU58DGQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the preprocessing steps\n","joblib.dump(pipe_test, 'preprocessor.joblib')"],"metadata":{"id":"cnZfNEZ09rab"},"execution_count":null,"outputs":[]}]}