{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1724473924429},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1724403013539},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1724329949643},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1724298383640},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1724139956220},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1724054788079},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723962163468},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723873090109},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723778725460},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723720997842},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723702539126},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723629465004},{"file_id":"1IG3rtuSryUSW9aJSzSv3Cj9Q3sjlK-hV","timestamp":1723606680546},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723550355534},{"file_id":"1TshDpWyZwGxEDeXPLl2EfPoWSD3Qu7aM","timestamp":1723525668579}],"collapsed_sections":["6U4S3d7fsRLq","hrPFXTgf0z2m","K6aKkPNA24wd","gWXVdU3rhoRK","UUPAWn7ktONj","I1DWjlExrRyn","TVgjxyQz01mx","TvlrWpey4jBW","xx72OX6cJeef","4YyR3UMbJlEt","NvBgnlC3Mgb9","1iHpN_dsu6ob","DuiIV2QMQJMw","kF98C-5GwPWy","7BMKGNEuwXy0","5w2ZGBx5HBXD","MLw8NXA0IIQV","A-y-VXdyIEjj","M1KAAAvL4ht-","4YRoJuM-0Lu7","3DtS8s2631xQ","tOkesqzLJQye","IubF6rcAJa9i","XFYgTGiPHmfQ","4e56KVp79hW1"],"authorship_tag":"ABX9TyMeyG6iDkbLvP845a1FrJOb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Configuring libraries & utilities"],"metadata":{"id":"6U4S3d7fsRLq"}},{"cell_type":"code","source":["# Upgrade scikit-learn\n","!pip install --upgrade scikit-learn -q\n","\n","# Install my custom module\n","!pip install git+https://github.com/Althaf9900/flash.git -q"],"metadata":{"id":"LAQIHv6GgIlF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standard Library Imports\n","import os\n","import json\n","import math\n","\n","# Google Drive Integration\n","from google.colab import drive\n","\n","# Data Manipulation and Preprocessing\n","import numpy as np\n","import pandas as pd\n","\n","# Data Visualization\n","import seaborn as sns\n","import scipy.stats as stats\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","\n","# Data Preprocessing and Transformation\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from imblearn.over_sampling import SMOTE\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import (\n","    FunctionTransformer, PowerTransformer, QuantileTransformer,\n","    StandardScaler, MinMaxScaler, RobustScaler,\n","    LabelEncoder, OneHotEncoder\n",")\n","\n","# Machine Learning Models\n","from sklearn.svm import SVC\n","from xgboost import XGBClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n","    VotingClassifier\n",")\n","\n","# Model Evaluation and Hyperparameter Tuning\n","from sklearn.model_selection import cross_val_score, GridSearchCV\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Saving\n","import joblib\n","\n","# Custom module for additional functionalities\n","import flash as fz"],"metadata":{"id":"I1_atIHsa4Br"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive if it's not already mounted\n","mount_point = \"/content/drive\"\n","\n","if not os.path.ismount(mount_point):\n","    print(\"Mounting Google Drive...\")\n","    drive.mount(mount_point)\n","else:\n","    print(\"Google Drive is already mounted.\")\n","\n","%cd /content/drive/MyDrive/Projects/loan-sanction-prediction"],"metadata":{"collapsed":true,"id":"WRozjl381lRa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Initial dataset assessment & preparation\n"],"metadata":{"id":"fcYXDXxxbYzn"}},{"cell_type":"code","source":["# Loading the dataset\n","df_copy = pd.read_csv('loan_sanction_train.csv')\n","df = df_copy # Keep a copy if needed"],"metadata":{"id":"GaiVFZ039qYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Understanding structure of the dataset\n","df.sample(5)"],"metadata":{"id":"--gXs5jR_W5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking whether Loan_ID contains duplicate IDs\n","df['Loan_ID'].duplicated().sum()"],"metadata":{"id":"eAjHUDF4VQ3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking whether the target feature is imbalanced or not\n","plt.pie(df['Loan_Status'].value_counts(), labels = df['Loan_Status'].unique(), autopct='%0.2f%%',\n","        shadow=True, explode=(0, 0.1), counterclock=False, colors=['lime', 'cyan'])\n","plt.show()"],"metadata":{"id":"ry1walf3AM55"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Columns of the dataset\n","print(df.columns)"],"metadata":{"id":"6GjPVOWM2il5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting some information about the dataset\n","df.info()"],"metadata":{"id":"lUPbGCTr_b20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dropping useless features that does not provide any predictive value to model training\n","df.drop('Loan_ID', axis=1, inplace=True)"],"metadata":{"id":"G2JG-P6ldwgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting numerical features & categorical features from the dataset using a custom made module\n","num_cols = fz.get_num_col(df)\n","cat_cols = fz.get_cat_col(df, ignore_cols=['Loan_Status'])\n","\n","# Print\n","print(num_cols)\n","print(cat_cols)"],"metadata":{"id":"8wFiCJgV_eIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count the number of categorical and numerical features\n","n_cat_cols = len(cat_cols)\n","n_num_cols = len(num_cols)\n","\n","# Print\n","print(f'Number of numerical features: {n_num_cols}')\n","print(f'Number of categorical features: {n_cat_cols}')"],"metadata":{"id":"ksSkcJ6tLSDz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (Before data cleaning)\n"],"metadata":{"id":"hrPFXTgf0z2m"}},{"cell_type":"markdown","source":["\n","## Outlier analysis\n"],"metadata":{"id":"K6aKkPNA24wd"}},{"cell_type":"code","source":["# Statistical measures\n","df[num_cols].describe().T"],"metadata":{"id":"TB8kLnYq4TRX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histogram & Box-plot\n","\n","def hist_box_plt(df, num_feature_list, figsize=None, title=None, hist_xlabel=None,\n","                hist_ylabel=None, box_xlabel=None, box_ylabel=None):\n","    # Number of features\n","    n_num_cols = len(num_feature_list)\n","\n","    # Subplots\n","    if not figsize and not isinstance(figsize, tuple):\n","        # Create subplots with dynamic figure size based on the number of numerical columns\n","        fig, axs = plt.subplots(n_num_cols, 2, figsize=(13, n_num_cols*3 + 1))\n","    else:\n","        fig, axs = plt.subplots(n_num_cols, 2, figsize=figsize)\n","\n","    # Plotting histograms and boxplots\n","    for i, col in enumerate(num_feature_list):\n","        # Histogram\n","        sns.histplot(df[col], kde=True, ax=axs[i, 0])\n","        axs[i, 0].set_title(f'Histogram of {col}')\n","        axs[i, 0].set_xlabel(hist_xlabel if hist_xlabel else '')\n","        axs[i, 0].set_ylabel(hist_ylabel if hist_ylabel else '')\n","        axs[i, 0].grid(True)\n","\n","        # Boxplot\n","        sns.boxplot(data=df, x=col, ax=axs[i, 1])\n","        axs[i, 1].set_title(f'Boxplot of {col}')\n","        axs[i, 1].set_xlabel(box_xlabel if box_xlabel else '')\n","        axs[i, 1].set_ylabel(box_ylabel if box_ylabel else '')\n","        axs[i, 1].grid(True)\n","\n","    # Adjust layout\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"CmE1HHsT4v0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hist_box_plt(df, num_cols)"],"metadata":{"id":"QMCp-GbgRQ_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Features with outliers\n","ftrs_with_outliers = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']"],"metadata":{"id":"BgqOq69_VSzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_outliers(df, features_with_outliers):\n","    outlier_df = pd.DataFrame()\n","    for feature in features_with_outliers:\n","        Q1 = df[feature].quantile(0.25)\n","        Q3 = df[feature].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","\n","        outliers = df[feature][(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n","\n","        # Add outliers to the DataFrame if any exist\n","        if not outliers.empty:\n","            outlier_df[feature] = outliers.sort_values().reset_index(drop=True)\n","\n","    return outlier_df"],"metadata":{"id":"ThX9xhP_YYIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outlier_df = find_outliers(df, ftrs_with_outliers)\n","outlier_df"],"metadata":{"id":"SqrXiCQzaGjX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- There are many outliers on the upper side of all numerical features.\n","\n","- None of the numerical features have outliers on the lower side.\n","\n","- Since we only have few data points, we can't afford to drop any data points.\n","    \n","- None of the numerical features follow a normal distribution.\n","\n","- The outliers appear to be valid and are not due to data entry issues.\n","\n","- Since the outliers are valid, apply capping methods, such as:\n","\n","    - Custom threshold capping: Set a threshold value based on analysis of the boxplots.\n","    - Percentile-based capping: Limit outliers to a specified percentile range.\n","    - Median imputation: Replace extreme values with the median.\n","\n","- After building the predictive model, evaluate the accuracy of all capping methods."],"metadata":{"id":"Nxw1gMdUdRoH"}},{"cell_type":"markdown","source":["\n","## Missing value analysis\n"],"metadata":{"id":"gWXVdU3rhoRK"}},{"cell_type":"code","source":["def calc_na_values(df, features, pct=True):\n","    # Count of missing values in features\n","    missing_value_count = df[features].isna().sum()\n","\n","    # Filter out features with no missing values\n","    missing_value_count = missing_value_count[missing_value_count > 0]\n","\n","    # Store features with missing values\n","    features_with_missing_values = missing_value_count.index.to_list()\n","\n","    if pct:\n","        # Percentage of missing values in features\n","        missing_value_pct = round(missing_value_count / df.shape[0] * 100, 2)\n","        return missing_value_pct, features_with_missing_values\n","    else:\n","        return missing_value_count, features_with_missing_values"],"metadata":{"id":"CuvHV-5UfIhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Numerical features\n","num_miss_pct, num_ftrs_with_na = calc_na_values(df, num_cols)\n","\n","# Percentage of missing values in numerical features\n","print(num_miss_pct)\n","\n","# Numerical features with missing values\n","print(num_ftrs_with_na)"],"metadata":{"id":"QFGp9zVfmzDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Categorical features\n","cat_miss_pct, cat_ftrs_with_na = calc_na_values(df, cat_cols)\n","\n","# Percentage of missing values in categorical features\n","print(cat_miss_pct)\n","\n","# Categorical features with missing values\n","print(cat_ftrs_with_na)"],"metadata":{"id":"7kkazocSnlj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def na_value_viz(df, figsize=None, cmap='Blues', xticks_rotation=None):\n","    if figsize:\n","        plt.figure(figsize=figsize)\n","    else:\n","        plt.figure(figsize=(15, 4))\n","    sns.heatmap(df.isna(), cbar=False, cmap=cmap, yticklabels=False)\n","    plt.xticks(rotation=xticks_rotation)\n","    plt.show()"],"metadata":{"id":"eQ5PC5X_s177"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing whether the missing values are missing at random or not\n","na_value_viz(df, xticks_rotation=45)"],"metadata":{"id":"buU_pHC2WwQg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting histogram of numerical features that have missing values to decide whether to use mean or median\n","plt.figure(figsize=(6, 4))\n","sns.histplot(df['LoanAmount'], kde=True)\n","plt.show()"],"metadata":{"id":"eWwW3tdPcD1c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- Only one numerical feature (['LoanAmount']) has missing values.\n","\n","- Six categorical features (['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History']) have missing values.\n","\n","- Since we only have few data points, we cannot afford to drop any data points.\n","\n","- The percentage of missing values is low across all features, so there is no need to drop any columns.\n","\n","- It appears that the missingness of values is random.\n","\n","- Missing value handling:\n","\n","    - Median imputation (for numerical features that are not normally distributed):\n","        - Loan amount\n","\n","    - Mode imputation:\n","        - Categorical features\n"],"metadata":{"id":"o0G6gChoKxfU"}},{"cell_type":"markdown","source":["\n","# Data Cleaning\n"],"metadata":{"id":"UUPAWn7ktONj"}},{"cell_type":"markdown","source":["Data cleaning steps:\n","\n","- Outlier handling:\n","\n","    - Since the outliers are valid, apply capping methods, such as:\n","\n","        - Custom threshold capping: Set a threshold value based on analysis of the boxplots.\n","        - Percentile-based capping: Limit outliers to a specified percentile range.\n","        - Median imputation: Replace outliers with the median.\n","\n","\n","- Missing value handling:\n","\n","    - Median imputation (for numerical features that are not normally distributed):\n","        - Loan amount\n","\n","    - Mode imputation:\n","        - Categorical features\n","\n","- Data type adjustments:\n","\n","    - Data type comapatibility:\n","        - Applicant income: float\n","        - Loan_Amount_Term & Credit_History: int then, str\n","\n","    - Memory usage optimization:\n","        - Categorical features: category"],"metadata":{"id":"rsFPXBYzVz27"}},{"cell_type":"code","source":["def custom_threshold_capping(df, features_with_outliers, cap_values):\n","    for i, feature in enumerate(ftrs_with_outliers):\n","        # Cap the values\n","        df[feature] = df[feature].clip(upper=cap_values[i])"],"metadata":{"id":"qK4hz9zKvYsi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Outlier handling: Custom threshold capping\n","\n","# Define the cap value\n","cap_values = [20833, 8980, 500]\n","custom_threshold_capping(df, ftrs_with_outliers, cap_values)"],"metadata":{"id":"arnnaMYTD-UE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Missing value handling\n","\n","# Imputing missing values in LoanAmount feature with median\n","median_imputer = SimpleImputer(strategy='median')\n","df['LoanAmount'] = median_imputer.fit_transform(df[['LoanAmount']])\n","\n","# Imputing missing values in categorical features with mode\n","mode_imputer = SimpleImputer(strategy='most_frequent')\n","df[cat_ftrs_with_na] = mode_imputer.fit_transform(df[cat_ftrs_with_na])\n","\n","# Test\n","if df.isna().sum().sum() == 0:\n","    print(\"There are no missing values left in the DataFrame.\")\n","else:\n","    print(\"There are still missing values in the DataFrame.\")"],"metadata":{"id":"0INL_0mEih5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking memory usage before dtype adjustments\n","print(\"Memory usage before adjustment:\", df.memory_usage(deep=True).sum())\n","print()\n","\n","# Data type adjustments\n","\n","# Data type compatibility\n","df['ApplicantIncome'] = df['ApplicantIncome'].astype(float)\n","\n","# Converting numerical categorical features to int and then to str\n","df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype(int).astype(str)\n","df['Credit_History'] = df['Credit_History'].astype(int).astype(str)\n","\n","# Memory usage optimization\n","df[cat_cols] = df[cat_cols].astype('category')\n","\n","# Print data types to confirm changes\n","print(df.dtypes)\n","\n","# Checking memory usage after dtype adjustments\n","print()\n","print(\"Memory usage after adjustment:\", df.memory_usage(deep=True).sum())"],"metadata":{"id":"kqpDRtGPlVJo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (On Independent features)\n"],"metadata":{"id":"I1DWjlExrRyn"}},{"cell_type":"markdown","source":["\n","## Univariate analysis\n"],"metadata":{"id":"TVgjxyQz01mx"}},{"cell_type":"markdown","source":["\n","### Numerical\n"],"metadata":{"id":"NgEA39dklZ8N"}},{"cell_type":"code","source":["# Statistical measures\n","df[num_cols].describe().T"],"metadata":{"id":"mDi3hcLNlfpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_stats_moments(df, features):\n","    stats_moments = pd.DataFrame(\n","        [\n","            {\n","                'Mean': round(float(df[col].mean()), 2),\n","                'Standard deviation': round(float(df[col].std()), 2),\n","                'Skewness': round(float(df[col].skew()), 2),\n","                'Kurtosis': round(float(df[col].kurtosis()), 2)\n","            }\n","            for col in features\n","        ],\n","        index=features\n","    )\n","    return stats_moments"],"metadata":{"id":"nETdpys7xveo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Statistical moments\n","calc_stats_moments(df, num_cols)"],"metadata":{"id":"QOOWv8wfp3Px"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting histogram & boxplot\n","hist_box_plt(df, num_cols)"],"metadata":{"id":"B4nhzoKGmhYz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- The distributions of applicant income and loan amount are right-skewed (positively skewed).\n","- Feature transformation is required for all numerical features to address this skewness.\n","- It looks like people with a co-applicant income of 0 don't have a co-applicant. So, we should create a new feature called 'Has_coapplicant'. For this feature, set the value to 'No' for individuals with a co-applicant income of 0, and 'Yes' for those with a non-zero co-applicant income."],"metadata":{"id":"-A0YZbXsnK3C"}},{"cell_type":"markdown","source":["\n","### Categorical\n"],"metadata":{"id":"cRUOZJ-l2h9s"}},{"cell_type":"code","source":["# Statistical measures\n","df[cat_cols].describe().T"],"metadata":{"id":"LkOlLNwQ32Q_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def countplots(df, features, n_cols=3, figsize=None, rotate_x_labels=None, rotation=45):\n","    n_features = len(features)\n","\n","    # Calculate number of rows needed for subplots\n","    n_rows = math.ceil(n_features / n_cols)\n","\n","    # Create subplots\n","    if not figsize:\n","        figsize=(n_cols * 4 + 1, n_rows * 3)\n","\n","    fig, axs = plt.subplots(n_rows, n_cols, figsize=figsize)\n","\n","    # Flatten axs array\n","    axs = axs.flatten()\n","\n","    # Plot countplots\n","    for i, feature in enumerate(features):\n","        sns.countplot(data=df, x=feature, ax=axs[i])\n","        axs[i].set_title(feature)\n","        axs[i].set_xlabel('')\n","        axs[i].set_ylabel('')\n","\n","        if feature in rotate_x_labels:\n","            axs[i].tick_params(axis='x', rotation=rotation)\n","\n","    # Turn off any unused subplots\n","    for j in range(n_features, len(axs)):\n","        axs[j].axis('off')\n","\n","    # Adjust layout\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"hbzZ5Ydg5q6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Countplots\n","countplots(df, cat_cols, rotate_x_labels=['Loan_Amount_Term'])"],"metadata":{"id":"ztNoCGw06mr9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- Males take out more loans compared to females.\n","- Married individuals take out more loans compared to - unmarried individuals.\n","- People without dependents take out more loans compared to those with dependents.\n","- Graduates take out more loans compared to non-graduates.\n","- Non-self-employed individuals take out more loans compared to self-employed individuals.\n","- Most people opt for a loan term of 360 months (30 years), followed by 180 months (15 years).\n","- People with a credit history of 1 take out more loans compared to those with a credit history of 0.\n","- People living in semi-urban areas take out more loans compared to those living in rural and urban areas. Rural residents take out the fewest loans. Although these relationships aren't strong, they may represent general trends."],"metadata":{"id":"Z1pkXN3GhmYk"}},{"cell_type":"markdown","source":["\n","## Bivariate analysis\n"],"metadata":{"id":"TvlrWpey4jBW"}},{"cell_type":"markdown","source":["\n","### Numerical - Numerical\n"],"metadata":{"id":"xx72OX6cJeef"}},{"cell_type":"code","source":["def hide_current_axis(*args, **kwds):\n","    plt.gca().set_visible(False)"],"metadata":{"id":"BwvUuj4sRvu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pairplot(df, features, kind='scatter', diag_kind='kde', plot_kws=None, fig_width=12.5,\n","             fig_height=None):\n","    n_features = len(features)\n","\n","    if not fig_height:\n","        fig_height = n_features + 3\n","\n","    height = fig_height / n_features\n","    aspect = fig_width / fig_height\n","\n","    if kind == 'reg' and plot_kws is None:\n","        plot_kws = {'line_kws':{'color':'red'}}\n","\n","    g = sns.pairplot(df[features], kind=kind, diag_kind=diag_kind, plot_kws=plot_kws,\n","                     height=height, aspect=aspect)\n","\n","    g.map_upper(hide_current_axis)\n","\n","    plt.show()"],"metadata":{"id":"5BGMkLM2ZSC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scatter plots with pairplot\n","pairplot(df, num_cols)"],"metadata":{"id":"GIcrLD4UCl4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Regplots with pairplot\n","pairplot(df, num_cols, kind='reg')"],"metadata":{"id":"B8ShB9vZA3tU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_custom_cmap(colors=[\"#FF0000\", \"#FFFF00\", \"#00FF00\"]):\n","    cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n","    return cmap"],"metadata":{"id":"rVyg0q9KS4GY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to create a mask for the upper triangle\n","def create_upper_triangle_mask(df, method):\n","    corr = df.corr(method=method)\n","    mask = np.triu(np.ones_like(corr, dtype=bool))\n","    np.fill_diagonal(mask, False)  # Optional: keep or remove diagonal elements\n","    return mask\n","\n","# Function to plot heatmap\n","def plot_corr_heatmap(df, method='pearson', cmap=None, title=None, ax=None):\n","    mask = create_upper_triangle_mask(df, method)\n","\n","    if cmap is None:\n","        cmap = create_custom_cmap()\n","\n","    sns.heatmap(df.corr(method=method), mask=mask, annot=True, cmap=cmap, ax=ax, cbar=False)\n","\n","    if title is None:\n","        if method in ['pearson', 'spearman']:\n","            title = f'{method.capitalize()} Correlation Heatmap'\n","        else:\n","            title = 'Correlation Heatmap'\n","\n","    ax.set_title(title)"],"metadata":{"id":"QS8prjqVxyGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(1, 3, figsize=(13, 5), gridspec_kw={'width_ratios': [1, 1, 0.05]})\n","\n","# Plot Pearson and Spearman heatmaps\n","plot_corr_heatmap(df[num_cols], ax=axs[0])\n","plot_corr_heatmap(df[num_cols], method='spearman', ax=axs[1])\n","\n","# Create a common colorbar for both heatmaps\n","cbar = fig.colorbar(axs[0].collections[0], cax=axs[2])\n","\n","# Adjust layout to prevent overlapping\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"WySczazSJ0AZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- None of the features show a strong linear relationship with each other. However, there is a moderate relationship between applicant income and loan amount. This makes sense because individuals with higher incomes often need larger loan amounts.\n","\n","- Both Pearson and Spearman correlation coefficients show similar patterns, but their values are slightly different. Since the heatmaps from both are similar, the exact values are less important. In this case, Spearman's correlation is more suitable because the data isn't normally distributed, doesn't have a linear relationship between features, and has outliers."],"metadata":{"id":"e87zhIkG2MKg"}},{"cell_type":"markdown","source":["\n","### Categorical - Categorical\n"],"metadata":{"id":"4YyR3UMbJlEt"}},{"cell_type":"code","source":["def crosstab_heatmap(df, features, target = None, cmap=None, fig_width=12.5,\n","                     fig_height=None, annot=True, cbar=False):\n","    if cmap is None:\n","        cmap = create_custom_cmap()\n","\n","    n_features = len(features)\n","    if target is None:\n","        n_plots = n_features * (n_features-1) // 2\n","    else:\n","        n_plots = n_features\n","\n","    # Automatically adjust fig_height if not provided\n","    if not fig_height:\n","        fig_height = n_plots * 5\n","\n","    fig, axs = plt.subplots(n_plots, 2, figsize=(fig_width, fig_height))\n","    axs = axs.reshape(-1, 2)  # Flatten the array of subplots\n","\n","    def plot_heatmaps(ax, table_index, table_column, title_index, title_column):\n","        sns.heatmap(table_index, annot=annot, cmap=cmap, cbar=cbar, fmt='0.2f',\n","                    xticklabels=True, yticklabels=True, ax=ax[0])\n","        sns.heatmap(table_column, annot=annot, cmap=cmap, cbar=cbar, fmt='0.2f',\n","                    xticklabels=True, yticklabels=True, ax=ax[1])\n","        ax[0].set_title(title_index)\n","        ax[1].set_title(title_column)\n","\n","    if target is None:\n","        plot_index = 0\n","        for i in range(n_features):\n","            for j in range(i + 1, n_features):\n","                table_index = pd.crosstab(df[features[i]], df[features[j]], normalize='index') * 100\n","                table_column = pd.crosstab(df[features[i]], df[features[j]], normalize='columns') * 100\n","                title_index = f\"{features[i]} vs {features[j]} (Index Normalized)\"\n","                title_column = f\"{features[i]} vs {features[j]} (Column Normalized)\"\n","                plot_heatmaps(axs[plot_index], table_index, table_column, title_index, title_column)\n","                plot_index += 1\n","    else:\n","        for i, feature in enumerate(features):\n","            table_index = pd.crosstab(df[feature], df[target], normalize='index') * 100\n","            table_column = pd.crosstab(df[feature], df[target], normalize='columns') * 100\n","            title_index = f\"{feature} vs {target} (Index Normalized)\"\n","            title_column = f\"{feature} vs {target} (Column Normalized)\"\n","            plot_heatmaps(axs[i], table_index, table_column, title_index, title_column)\n","\n","    # Adjust layout\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"ayvpFoZBnHrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Heatmap\n","crosstab_heatmap(df, cat_cols)"],"metadata":{"id":"-FdPARm2oevL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Numerical - Categorical\n"],"metadata":{"id":"NvBgnlC3Mgb9"}},{"cell_type":"code","source":["def num_cat_analysis(df, num_feature, cat_features, fig_width=13, fig_height=None,\n","                     mean_color='blue', median_color='red'):\n","    n_cat_features = len(cat_features)\n","\n","    # Set default figure height based on number of categorical features\n","    if not fig_height:\n","        fig_height = n_cat_features * 4\n","\n","    fig, axs = plt.subplots(n_cat_features, 2, figsize=(fig_width, fig_height))\n","\n","    for i, cat_feature in enumerate(cat_features):\n","        # Plot violinplot\n","        sns.violinplot(df, x=cat_feature, y=num_feature, hue=cat_feature,\n","                       ax=axs[i, 0])\n","        axs[i, 0].set_title(f'Violinplot of {num_feature} by {cat_feature}')\n","\n","        # Plot pointplot\n","        sns.pointplot(df, x=cat_feature, y=num_feature, errorbar=None, color=mean_color,\n","                      ax=axs[i, 1], label='Mean')\n","        sns.pointplot(df, x=cat_feature, y=num_feature, errorbar=None, color=median_color,\n","                      estimator='median', ax=axs[i, 1], label='Median')\n","        axs[i, 1].set_title(f'Pointplot of {num_feature} by {cat_feature}')\n","\n","    # Adjust layout\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"ISJsomzfuqEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applicant income\n","num_cat_analysis(df, 'ApplicantIncome', cat_cols)"],"metadata":{"id":"E8i5SjM5vNuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Coapplicant income\n","num_cat_analysis(df, 'CoapplicantIncome', cat_cols)"],"metadata":{"id":"j_PPBJqN9O_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loan amount\n","num_cat_analysis(df, 'LoanAmount', cat_cols)"],"metadata":{"id":"gyNZz0No9SFq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Feature construction\n"],"metadata":{"id":"1iHpN_dsu6ob"}},{"cell_type":"markdown","source":["- It appears that individuals with a co-applicant income of 0 do not have a co-applicant. Therefore, create a feature named 'Has_coapplicant'. In this feature, set individuals with a co-applicant income of 0 to 'No', and those with a non-zero co-applicant income to 'Yes'."],"metadata":{"id":"DilMLweTueyk"}},{"cell_type":"code","source":["df['Has_coapplicant'] = np.where(df['CoapplicantIncome'] == 0, 'No', 'Yes')\n","df['Has_coapplicant']"],"metadata":{"id":"Op8pR4w8kIr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Appending newly constructed features to feature lists based on their type\n","cat_cols.append('Has_coapplicant')"],"metadata":{"id":"EslfarRnJkzN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (On newly constructed features)\n"],"metadata":{"id":"DuiIV2QMQJMw"}},{"cell_type":"markdown","source":["\n","## Univariate analysis\n"],"metadata":{"id":"kF98C-5GwPWy"}},{"cell_type":"code","source":["df['Has_coapplicant'].describe().T"],"metadata":{"id":"TLhb427hwUCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.countplot(x = df['Has_coapplicant'])\n","plt.show()"],"metadata":{"id":"dAiY4fGRwXHK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Bivariate analysis\n"],"metadata":{"id":"7BMKGNEuwXy0"}},{"cell_type":"markdown","source":["\n","#### Categorical - Categorical"],"metadata":{"id":"pQteagYGy5b_"}},{"cell_type":"code","source":["# Heatmap\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_cat_cols, 2, figsize=(13, 40))  # Adjust size as needed\n","axs = axs.reshape(-1, 2)  # Flatten the array of subplots\n","\n","# Plot heatmaps\n","for i in range(n_cat_cols):\n","    # Create contingency tables\n","    contingency_table_1 = pd.crosstab(df[cat_cols[i]], df['Has_coapplicant'], normalize='index') * 100\n","    contingency_table_2 = pd.crosstab(df[cat_cols[i]], df['Has_coapplicant'], normalize='columns') * 100\n","\n","    # Plot heatmaps\n","    sns.heatmap(contingency_table_1, annot=True, cmap=create_custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 0])\n","    sns.heatmap(contingency_table_2, annot=True, cmap=create_custom_cmap, cbar=False, fmt='0.2f',\n","                xticklabels=True, yticklabels=True, ax=axs[i, 1])\n","\n","    # Set titles for each subplot\n","    axs[i, 0].set_title(f\"{cat_cols[i]} vs Has_coapplicant (Index Normalized)\")\n","    axs[i, 1].set_title(f\"{cat_cols[i]} vs Has_coapplicant (Column Normalized)\")\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"pIGdHXidlZEw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### Numerical - Categorical\n"],"metadata":{"id":"LJ7rmAjWzHzZ"}},{"cell_type":"code","source":["# Violinplot & Pointplot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 9))\n","\n","for i in range(n_num_cols):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x='Has_coapplicant', y=num_cols[i], ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_cols[i]} by Has_coapplicant')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x='Has_coapplicant', y=num_cols[i], errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x='Has_coapplicant', y=num_cols[i], errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_cols[i]} by Has_coapplicant')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TUk79SjZy3oM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# EDA (Between independent features and target feature)"],"metadata":{"id":"5w2ZGBx5HBXD"}},{"cell_type":"markdown","source":["\n","## Categorical - Categorical\n"],"metadata":{"id":"MLw8NXA0IIQV"}},{"cell_type":"code","source":["crosstab_heatmap(df, cat_cols, target='Loan_Status')"],"metadata":{"id":"WgbeDDifHiQ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Numerical - Categorical\n"],"metadata":{"id":"A-y-VXdyIEjj"}},{"cell_type":"code","source":["def num_cat_analysis(df, num_feature, cat_feature, fig_width=13, fig_height=None,\n","                     mean_color='blue', median_color='red'):\n","    if isinstance(num_feature, list) and isinstance(cat_feature, str):\n","        n_features = len(num_feature)\n","\n","        # Set default figure height based on number of categorical features\n","        if not fig_height:\n","            fig_height = n_features * 4\n","\n","        fig, axs = plt.subplots(n_features, 2, figsize=(fig_width, fig_height))\n","\n","        for i, feature in enumerate(num_feature):\n","            # Plot violinplot\n","            sns.violinplot(df, x=cat_feature, y=feature, hue=cat_feature,\n","                        ax=axs[i, 0])\n","            axs[i, 0].set_title(f'Violinplot of {feature} by {cat_feature}')\n","\n","            # Plot pointplot\n","            sns.pointplot(df, x=cat_feature, y=feature, errorbar=None, color=mean_color,\n","                        ax=axs[i, 1], label='Mean')\n","            sns.pointplot(df, x=cat_feature, y=feature, errorbar=None, color=median_color,\n","                        estimator='median', ax=axs[i, 1], label='Median')\n","            axs[i, 1].set_title(f'Pointplot of {feature} by {cat_feature}')\n","    elif isinstance(cat_feature, list) and isinstance(num_feature, str):\n","        n_features = len(cat_feature)\n","\n","        # Set default figure height based on number of categorical features\n","        if not fig_height:\n","            fig_height = n_features * 4\n","\n","        fig, axs = plt.subplots(n_features, 2, figsize=(fig_width, fig_height))\n","\n","        for i, feature in enumerate(cat_feature):\n","            # Plot violinplot\n","            sns.violinplot(df, x=feature, y=num_feature, hue=feature,\n","                        ax=axs[i, 0])\n","            axs[i, 0].set_title(f'Violinplot of {feature} by {num_feature}')\n","\n","            # Plot pointplot\n","            sns.pointplot(df, x=feature, y=num_feature, errorbar=None, color=mean_color,\n","                        ax=axs[i, 1], label='Mean')\n","            sns.pointplot(df, x=feature, y=num_feature, errorbar=None, color=median_color,\n","                        estimator='median', ax=axs[i, 1], label='Median')\n","            axs[i, 1].set_title(f'Pointplot of {feature} by {num_feature}')\n","\n","    # Adjust layout\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"kWbZHaYccomZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_cat_analysis(df, num_feature=num_cols, cat_feature='Loan_Status')"],"metadata":{"id":"x67BJTRukXcj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_cat_analysis(df, cat_feature=cat_cols, num_feature='ApplicantIncome')"],"metadata":{"id":"BhaPGHh_ddJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Violinplot & Pointplot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, 2, figsize=(12, 9))\n","\n","for i in range(n_num_cols):\n","\n","    # Plot violinplot\n","    sns.violinplot(data=df, x='Loan_Status', y=num_cols[i], ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Violinplot of {num_cols[i]} by Loan_Status')\n","\n","    # Plot pointplot\n","    sns.pointplot(data=df, x='Loan_Status', y=num_cols[i], errorbar=None, color='blue', ax=axs[i, 1])\n","    sns.pointplot(data=df, x='Loan_Status', y=num_cols[i], errorbar=None, color='red', estimator='median', ax=axs[i, 1])\n","    axs[i, 1].set_title(f'Pointplot of {num_cols[i]} by Loan_Status')\n","    handles = [\n","        plt.Line2D([0], [0], color='blue', marker='o', markersize=8, label='Mean'),\n","        plt.Line2D([0], [0], color='red', marker='o', markersize=8, label='Median')\n","    ]\n","    axs[i, 1].legend(handles=handles, title='Estimation type')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"L-a5UIaJHp-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Feature transformation\n"],"metadata":{"id":"M1KAAAvL4ht-"}},{"cell_type":"code","source":["# Epsilon to avoid log(0) and sqrt(0)\n","epsilon = 1e-10\n","\n","# Initialize transformers\n","transformers = {\n","    'Log': FunctionTransformer(func=lambda X: np.log(X + epsilon), validate=False),\n","    'Square Root': FunctionTransformer(func=lambda X: np.sqrt(X + epsilon),\n","                                       validate=False),\n","    'Square': FunctionTransformer(func=np.square, validate=False),\n","    'Reciprocal': FunctionTransformer(func=lambda X: np.reciprocal(X + epsilon),\n","                                      validate=False),\n","    'Yeo-Johnson': PowerTransformer(standardize=False),\n","    'Quantile': QuantileTransformer(n_quantiles=df.shape[0], output_distribution='normal')\n","}\n","\n","# Apply transformations\n","transformed_data = {}\n","for name, transformer in transformers.items():\n","    transformed_data[name] = pd.DataFrame(transformer.fit_transform(df[num_cols]),\n","                                          columns=num_cols)"],"metadata":{"id":"SAregm_b1l0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histogram\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, len(transformers) + 1, figsize=(26, 9))\n","\n","# Plot histograms\n","for i, col in enumerate(num_cols):\n","    # Original\n","    sns.histplot(df[col], kde=True, ax=axs[i, 0])\n","    axs[i, 0].set_title(f'Original {col}')\n","\n","    for j, (name, transformed_df) in enumerate(transformed_data.items()):\n","        sns.histplot(transformed_df[col], kde=True, ax=axs[i, j + 1])\n","        axs[i, j + 1].set_title(f'{name} {col}')\n","\n","# Turn off y axis labels for all subplots\n","axs = axs.flatten()\n","for ax in axs:\n","    ax.set_ylabel('')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"gmF2YBK-zsNW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# QQ plot\n","\n","# Create subplots\n","fig, axs = plt.subplots(n_num_cols, len(transformers) + 1, figsize=(26, 9))\n","\n","# Plot Q-Q plots\n","for i, col in enumerate(num_cols):\n","    # Original\n","    stats.probplot(df[col], dist=\"norm\", plot=axs[i, 0])\n","    axs[i, 0].set_title(f'Original {col}')\n","    axs[i, 0].get_lines()[1].set_color('red')  # Make the reference line red\n","\n","    for j, (name, transformed_df) in enumerate(transformed_data.items()):\n","        stats.probplot(transformed_df[col], dist=\"norm\", plot=axs[i, j + 1])\n","        axs[i, j + 1].set_title(f'{name} {col}')\n","        axs[i, j + 1].get_lines()[1].set_color('red')  # Make the reference line red\n","\n","# Turn off x and y axis labels for all subplots\n","axs = axs.flatten()\n","for ax in axs:\n","    ax.set_xlabel('')\n","    ax.set_ylabel('')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"dm5_EpPm1_9M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","- ApplicantIncome & LoanAmount: Quantile Transform\n","- CoapplicantIncome: Reciprocal Transform"],"metadata":{"id":"vK6XEpnv-ADI"}},{"cell_type":"code","source":["df['ApplicantIncome'] = transformed_data['Quantile']['ApplicantIncome']\n","df['CoapplicantIncome'] = transformed_data['Reciprocal']['CoapplicantIncome']\n","df['LoanAmount'] = transformed_data['Quantile']['LoanAmount']"],"metadata":{"id":"CrtowETC-sem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Building predictive model"],"metadata":{"id":"4YRoJuM-0Lu7"}},{"cell_type":"markdown","source":["\n","## Preparing the data\n"],"metadata":{"id":"3DtS8s2631xQ"}},{"cell_type":"code","source":["# Splitting the data into features and target\n","X_train = df.drop('Loan_Status', axis=1)\n","X_train = X_train[num_cols + cat_cols] # To place numerical columns first\n","y_train = df['Loan_Status']"],"metadata":{"id":"cWfE79OQpVSU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Label encode the target feature\n","\n","# Initialize the LabelEncoder\n","le = LabelEncoder()\n","\n","# Fit and transform the target feature\n","y_train = le.fit_transform(y_train)"],"metadata":{"id":"RcbjjeDIjRfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting indices of numerical and categorical features of the X_train dataframe\n","indices_num_cols = X_train.columns.get_indexer(num_cols)\n","indices_cat_cols = X_train.columns.get_indexer(cat_cols)"],"metadata":{"id":"6tDq_HzbrwSw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Column transformer for preprocessing\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('scaler', StandardScaler(), indices_num_cols), # Scaling numerical columns\n","        ('encoder', OneHotEncoder(drop='first', sparse_output=False), indices_cat_cols) # OneHotEncoding categorical columns\n","    ],\n","    remainder='passthrough'  # Keep the columns not listed in num_cols or cat_cols as is\n",")"],"metadata":{"id":"PtX68o5CEp5O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pipeline\n","pipe = Pipeline([\n","    ('preprocessor', preprocessor)\n","])"],"metadata":{"id":"9p0MyzCC_tCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing and Transforming training data using pipeline\n","X_train_transformed = pipe.fit_transform(X_train)"],"metadata":{"id":"I5r6wuybIz6p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Handling imbalanced dataset\n"],"metadata":{"id":"k9rg3p9HeW2p"}},{"cell_type":"code","source":["# Oversampling the dataset using SMOTE\n","smote = SMOTE(random_state=42)  # Initialize SMOTE with optional random_state for reproducibility\n","X_train_transformed, y_train = smote.fit_resample(X_train_transformed, y_train)"],"metadata":{"id":"tn53K6SIec76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test\n","unique_values, counts = np.unique(y_train, return_counts=True)\n","\n","# Print the counts of each class\n","for value, count in zip(unique_values, counts):\n","    print(f\"Class {value}: {count}\")"],"metadata":{"id":"LNYiB3gYegOn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Model selection (Before hyperparameter tuning)\n"],"metadata":{"id":"tOkesqzLJQye"}},{"cell_type":"code","source":["# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression,\n","    'Random Forest': RandomForestClassifier,\n","    'Gradient Boosting': GradientBoostingClassifier,\n","    'Support Vector Machine': SVC,\n","    'KNN': KNeighborsClassifier,\n","    'Decision Trees': DecisionTreeClassifier,\n","    'Xgboost': XGBClassifier,\n","    'Extra Trees': ExtraTreesClassifier\n","}\n","# Define metric functions\n","metrics = {\n","    'accuracy': accuracy_score,\n","    'precision': precision_score,\n","    'recall': recall_score,\n","    'f1': f1_score\n","}"],"metadata":{"id":"MQgmPbGs9Qwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval_models_across_metrics(models, metrics, X_train, y_train, cv=5, sort=False,\n","                               model_params=None):\n","\n","    models_across_metrics = {metric: {} for metric in metrics}\n","\n","    for metric in metrics:\n","        for model_name, model in models.items():\n","\n","            if isinstance(model_params, dict):\n","                cv_scores = cross_val_score(model(**model_params[model_name]), X_train,\n","                                            y_train, cv=cv, scoring=metric)\n","            else:\n","                cv_scores = cross_val_score(model(), X_train,\n","                                            y_train, cv=cv, scoring=metric)\n","\n","            cv_scores_mean = cv_scores.mean()\n","\n","            models_across_metrics[metric][model_name] = round(cv_scores_mean, 3)\n","\n","    if sort:\n","        for metric, model_scores in models_across_metrics.items():\n","            models_across_metrics[metric] = dict(\n","                sorted(model_scores.items(), key=lambda item: item[1], reverse=True)\n","                )\n","\n","    return models_across_metrics"],"metadata":{"id":"NCXNRkQqoQcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Comparing models across metrics\n","models_across_metrics = eval_models_across_metrics(models, metrics.keys(), X_train_transformed,\n","                                                   y_train, sort=True)\n","print(json.dumps(models_across_metrics, indent=4))"],"metadata":{"id":"ARzQsfocvwGQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusions:\n","\n","After evaluating the metrics, I have decided to focus on the top 3 models: Random Forest Classifier, Extra Trees Classifier, Xgboost Classifier. These models have demonstrated strong performance across the different metrics, making them the best candidates for further fine-tuning and optimization."],"metadata":{"id":"9_21ZbTWH8dq"}},{"cell_type":"markdown","source":["\n","## Hyperparameter tuning\n"],"metadata":{"id":"IubF6rcAJa9i"}},{"cell_type":"code","source":["# Define top models for further hyperparameter tuning\n","models = {\n","    'Random Forest': RandomForestClassifier,\n","    'Xgboost': XGBClassifier,\n","    'Extra Trees': ExtraTreesClassifier\n","}"],"metadata":{"id":"AfG2BpNjKF4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define parameter grids\n","param_grids = {\n","    'Random Forest': {\n","        'n_estimators': [50, 100, 200],\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4]\n","    },\n","    'Xgboost': {\n","        'n_estimators': [50, 100, 200],\n","        'max_depth': [3, 6, 10],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'subsample': [0.8, 0.9, 1.0],\n","        'colsample_bytree': [0.8, 0.9, 1.0],\n","        'gamma': [0, 0.1, 0.2]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [50, 100, 200],\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'bootstrap': [True, False]\n","    }\n","}"],"metadata":{"id":"nv1r5gjDTo_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hyperparameter_tuning(models_dict = None, param_grids = None, X_train = None,\n","                          y_train = None, file_path=None, force_overwrite = False):\n","    def perform_grid_search(models_dict, param_grids, X_train, y_train):\n","        best_params = {}\n","        for model_name, model in models_dict.items():\n","            print(f\"Processing {model_name}...\")\n","            param_grid = param_grids[model_name]\n","            grid_search = GridSearchCV(estimator=model(), param_grid=param_grid,\n","                                    scoring='accuracy', cv=5, n_jobs=-1)\n","            grid_search.fit(X_train, y_train)\n","            best_params[model_name] = {\n","                'Best Parameters': grid_search.best_params_,\n","                'Average accuracy score on the best parameters': round(grid_search.best_score_, 3)\n","            }\n","        return best_params\n","\n","    if file_path:\n","        if os.path.exists(file_path) and not force_overwrite:\n","            best_params = joblib.load(file_path)\n","        else:\n","            best_params = perform_grid_search(models_dict, param_grids, X_train, y_train)\n","            joblib.dump(best_params, file_path)\n","    else:\n","        best_params = perform_grid_search(models_dict, param_grids, X_train, y_train)\n","\n","    return best_params"],"metadata":{"id":"yvIm51meOyWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finding best hyperparameters on top models using GridSearchCV\n","best_params = hyperparameter_tuning(file_path='best_params.joblib')\n","print(json.dumps(best_params, indent=4))"],"metadata":{"id":"uyYCLkcQpMrP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Comparing top models across metrics after hyperparameter tuning\n","models_across_metrics = eval_models_across_metrics(models, metrics.keys(), X_train_transformed,\n","                                                   y_train, sort=True, model_params=best_params)\n","print(json.dumps(models_across_metrics, indent=4))"],"metadata":{"id":"-YCeVOVMmSW2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Model training\n"],"metadata":{"id":"XFYgTGiPHmfQ"}},{"cell_type":"code","source":["estimators = []\n","for model_name, model in models.items():\n","    estimators.append((model_name, model(**best_params[model_name]['Best Parameters'])))"],"metadata":{"id":"CgYhwKovD2iZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval_voting_clf(estimators, X_train, y_train, cv = 5):\n","    # Create a voting classifier (hard voting)\n","    voting_clf_hard = VotingClassifier(estimators=estimators, voting='hard')\n","\n","    # Create a voting classifier (soft voting)\n","    voting_clf_soft = VotingClassifier(estimators=estimators, voting='soft')\n","\n","    # Apply cross-validation\n","    cv_scores_h = cross_val_score(voting_clf_hard, X_train, y_train, cv=cv, scoring='accuracy')\n","    cv_scores_s = cross_val_score(voting_clf_soft, X_train, y_train, cv=cv, scoring='accuracy')\n","\n","    accuracy_results = {}\n","\n","    accuracy_results['Hard Margin'] = round(cv_scores_h.mean(), 3)\n","    accuracy_results['Soft Margin'] = round(cv_scores_s.mean(), 3)\n","\n","    return accuracy_results"],"metadata":{"id":"mCVRtfc_pPcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accuracy on hard and soft margin voting classifiers\n","accuracy = eval_voting_clf(estimators, X_train_transformed, y_train)\n","accuracy"],"metadata":{"id":"V9ktDp80CxPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the best model\n","voting_clf = VotingClassifier(estimators, voting='hard')\n","voting_clf.fit(X_train_transformed, y_train)"],"metadata":{"id":"JgkcMpQkiev4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving"],"metadata":{"id":"4e56KVp79hW1"}},{"cell_type":"code","source":["# Save the Machine Learning model\n","joblib.dump(voting_clf, 'model.joblib')"],"metadata":{"id":"Yzw_aU58DGQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the preprocessing steps\n","joblib.dump(pipe, 'preprocessor.joblib')"],"metadata":{"id":"cnZfNEZ09rab"},"execution_count":null,"outputs":[]}]}